{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.func import jacrev\n",
    "\n",
    "class lnn(torch.nn.Module):\n",
    "    def __init__(self, env_name, n, obs_size, action_size, dt, a_zeros):\n",
    "        super(lnn, self).__init__()\n",
    "        self.env_name = env_name\n",
    "        self.dt = dt\n",
    "        self.n = n\n",
    "\n",
    "        input_size = obs_size - self.n\n",
    "        out_L = int(self.n*(self.n+1)/2)\n",
    "        self.fc1_L = torch.nn.Linear(input_size, 64)\n",
    "        self.fc2_L = torch.nn.Linear(64, 64)\n",
    "        self.fc3_L = torch.nn.Linear(64, out_L)\n",
    "        if not self.env_name == \"reacher\":\n",
    "            self.fc1_V = torch.nn.Linear(input_size, 64)\n",
    "            self.fc2_V = torch.nn.Linear(64, 64)\n",
    "            self.fc3_V = torch.nn.Linear(64, 1)\n",
    "        print(a_zeros.shape, n, action_size, obs_size)\n",
    "        self.a_zeros = a_zeros\n",
    "\n",
    "    def trig_transform_q(self, q):\n",
    "        if self.env_name == \"pendulum\":\n",
    "            return torch.column_stack((torch.cos(q[:,0]),torch.sin(q[:,0])))\n",
    "        \n",
    "    \n",
    "        elif self.env_name == \"reacher\" or self.env_name == \"acrobot\":\n",
    "            return torch.column_stack((torch.cos(q[:,0]),torch.sin(q[:,0]),\\\n",
    "                                       torch.cos(q[:,1]),torch.sin(q[:,1])))\n",
    "        \n",
    "        elif self.env_name == \"cartpole\":\n",
    "            return torch.column_stack((q[:,0],\\\n",
    "                                       torch.cos(q[:,1]),torch.sin(q[:,1])))\n",
    "        \n",
    "        elif self.env_name == \"cart2pole\":\n",
    "            return torch.column_stack((q[:,0],\\\n",
    "                                       torch.cos(q[:,1]),torch.sin(q[:,1]),\\\n",
    "                                       torch.cos(q[:,2]),torch.sin(q[:,2])))\n",
    "\n",
    "        elif self.env_name == \"cart3pole\":\n",
    "            return torch.column_stack((q[:,0],\\\n",
    "                                       torch.cos(q[:,1]),torch.sin(q[:,1]),\\\n",
    "                                       torch.cos(q[:,2]),torch.sin(q[:,2]),\\\n",
    "                                       torch.cos(q[:,3]),torch.sin(q[:,3])))\n",
    "        \n",
    "        elif self.env_name == \"acro3bot\":\n",
    "            return torch.column_stack((torch.cos(q[:,0]),torch.sin(q[:,0]),\\\n",
    "                                       torch.cos(q[:,1]),torch.sin(q[:,1]),\\\n",
    "                                       torch.cos(q[:,2]),torch.sin(q[:,2])))\n",
    "\n",
    "    def inverse_trig_transform_model(self, x):\n",
    "        if self.env_name == \"pendulum\":\n",
    "            return torch.cat((torch.atan2(x[:,1],x[:,0]).unsqueeze(1),x[:,2:]),1)\n",
    "        \n",
    "        elif self.env_name == \"reacher\" or self.env_name == \"acrobot\":\n",
    "            return torch.cat((torch.atan2(x[:,1],x[:,0]).unsqueeze(1),torch.atan2(x[:,3],x[:,2]).unsqueeze(1),x[:,4:]),1)\n",
    "        \n",
    "        elif self.env_name == \"cartpole\":\n",
    "            return torch.cat((x[:,0].unsqueeze(1),torch.atan2(x[:,2],x[:,1]).unsqueeze(1),x[:,3:]),1)\n",
    "        \n",
    "        elif self.env_name == \"cart2pole\":\n",
    "            return torch.cat((x[:,0].unsqueeze(1),torch.atan2(x[:,2],x[:,1]).unsqueeze(1),torch.atan2(x[:,4],x[:,3]).unsqueeze(1),x[:,5:]),1)\n",
    "\n",
    "        elif self.env_name == \"cart3pole\":\n",
    "            return torch.cat((x[:,0].unsqueeze(1),torch.atan2(x[:,2],x[:,1]).unsqueeze(1),torch.atan2(x[:,4],x[:,3]).unsqueeze(1),\n",
    "                              torch.atan2(x[:,6],x[:,5]).unsqueeze(1),x[:,7:]),1)\n",
    "\n",
    "        elif self.env_name == \"acro3bot\":\n",
    "            return torch.cat((torch.atan2(x[:,1],x[:,0]).unsqueeze(1),torch.atan2(x[:,3],x[:,2]).unsqueeze(1),torch.atan2(x[:,5],x[:,4]).unsqueeze(1),\n",
    "                              x[:,6:]),1)\n",
    "\n",
    "    def compute_L(self, q):\n",
    "        y1_L = F.softplus(self.fc1_L(q))\n",
    "        y2_L = F.softplus(self.fc2_L(y1_L))\n",
    "        y_L = self.fc3_L(y2_L)\n",
    "        device = y_L.device\n",
    "        if self.n == 1:\n",
    "            L = y_L.unsqueeze(1)\n",
    "\n",
    "        elif self.n == 2:\n",
    "            L11 = y_L[:,0].unsqueeze(1)\n",
    "            L1_zeros = torch.zeros(L11.size(0),1, dtype=torch.float32, device=device)\n",
    "\n",
    "            L21 = y_L[:,1].unsqueeze(1)\n",
    "            L22 = y_L[:,2].unsqueeze(1)\n",
    "\n",
    "            L1 = torch.cat((L11,L1_zeros),1) \n",
    "            L2 = torch.cat((L21,L22),1)\n",
    "            L = torch.cat((L1.unsqueeze(1),L2.unsqueeze(1)),1)\n",
    "        \n",
    "        elif self.n == 3:\n",
    "            L11 = y_L[:,0].unsqueeze(1)\n",
    "            L1_zeros = torch.zeros(L11.size(0),2, dtype=torch.float32, device=device)\n",
    "\n",
    "            L21 = y_L[:,1].unsqueeze(1)\n",
    "            L22 = y_L[:,2].unsqueeze(1)\n",
    "            L2_zeros = torch.zeros(L21.size(0),1, dtype=torch.float32, device=device)\n",
    "\n",
    "            L31 = y_L[:,3].unsqueeze(1)\n",
    "            L32 = y_L[:,4].unsqueeze(1)\n",
    "            L33 = y_L[:,5].unsqueeze(1)\n",
    "\n",
    "            L1 = torch.cat((L11,L1_zeros),1) \n",
    "            L2 = torch.cat((L21,L22,L2_zeros),1)\n",
    "            L3 = torch.cat((L31,L32,L33),1)\n",
    "            L = torch.cat((L1.unsqueeze(1),L2.unsqueeze(1),L3.unsqueeze(1)),1)\n",
    "        \n",
    "        elif self.n == 4:\n",
    "            L11 = y_L[:,0].unsqueeze(1)\n",
    "            L1_zeros = torch.zeros(L11.size(0),3, dtype=torch.float32, device=device)\n",
    "\n",
    "            L21 = y_L[:,1].unsqueeze(1)\n",
    "            L22 = y_L[:,2].unsqueeze(1)\n",
    "            L2_zeros = torch.zeros(L21.size(0),2, dtype=torch.float32, device=device)\n",
    "\n",
    "            L31 = y_L[:,3].unsqueeze(1)\n",
    "            L32 = y_L[:,4].unsqueeze(1)\n",
    "            L33 = y_L[:,5].unsqueeze(1)\n",
    "            L3_zeros = torch.zeros(L31.size(0),1, dtype=torch.float32, device=device)\n",
    "\n",
    "            L41 = y_L[:,6].unsqueeze(1)\n",
    "            L42 = y_L[:,7].unsqueeze(1)\n",
    "            L43 = y_L[:,8].unsqueeze(1)\n",
    "            L44 = y_L[:,9].unsqueeze(1)\n",
    "\n",
    "            L1 = torch.cat((L11,L1_zeros),1) \n",
    "            L2 = torch.cat((L21,L22,L2_zeros),1)\n",
    "            L3 = torch.cat((L31,L32,L33,L3_zeros),1)\n",
    "            L4 = torch.cat((L41,L42,L43,L44),1)\n",
    "            L = torch.cat((L1.unsqueeze(1),L2.unsqueeze(1),L3.unsqueeze(1),L4.unsqueeze(1)),1)\n",
    "\n",
    "        return L\n",
    "\n",
    "    def get_A(self, a):\n",
    "        if self.env_name == \"pendulum\" or self.env_name == \"reacher\":\n",
    "            A = a\n",
    "        \n",
    "        elif self.env_name == \"acrobot\":\n",
    "            A = torch.cat((self.a_zeros,a),1)\n",
    "        \n",
    "        elif self.env_name == \"cartpole\" or self.env_name == \"cart2pole\":\n",
    "            A = torch.cat((a,self.a_zeros),1)\n",
    "        \n",
    "        elif self.env_name == \"cart3pole\" or self.env_name == \"acro3bot\":\n",
    "            A = torch.cat((a[:,:1],self.a_zeros,a[:,1:]),1)\n",
    "\n",
    "        return A\n",
    "\n",
    "    def get_L(self, q):\n",
    "        trig_q = self.trig_transform_q(q)\n",
    "        L = self.compute_L(trig_q)         \n",
    "        return L.sum(0), L\n",
    "\n",
    "    def get_V(self, q):\n",
    "        trig_q = self.trig_transform_q(q)\n",
    "        y1_V = F.softplus(self.fc1_V(trig_q))\n",
    "        y2_V = F.softplus(self.fc2_V(y1_V))\n",
    "        V = self.fc3_V(y2_V).squeeze()\n",
    "        return V.sum()\n",
    "\n",
    "    def get_acc(self, q, qdot, a):\n",
    "        dL_dq, L = jacrev(self.get_L, has_aux=True)(q)\n",
    "        term_1 = torch.einsum('blk,bijk->bijl', L, dL_dq.permute(2,3,0,1))\n",
    "        dM_dq = term_1 + term_1.transpose(2,3)\n",
    "        c = torch.einsum('bjik,bk,bj->bi', dM_dq, qdot, qdot) - 0.5 * torch.einsum('bikj,bk,bj->bi', dM_dq, qdot, qdot)        \n",
    "        Minv = torch.cholesky_inverse(L)\n",
    "        dV_dq = 0 if self.env_name == \"reacher\" else jacrev(self.get_V)(q)\n",
    "        qddot = torch.matmul(Minv,(self.get_A(a)-c-dV_dq).unsqueeze(2)).squeeze(2)\n",
    "        return qddot                                                                                                                                                                                                                                                                                                                 \n",
    "                                                                                                                                                                                                           \n",
    "    def derivs(self, s, a):\n",
    "        q, qdot = s[:,:self.n], s[:,self.n:]\n",
    "        qddot = self.get_acc(q, qdot, a)\n",
    "        return torch.cat((qdot,qddot),dim=1)                                                                                                                                                               \n",
    "\n",
    "    def rk2(self, s, a):                                                                                                                                                                                   \n",
    "        alpha = 2.0/3.0 # Ralston's method                                                                                                                                                                 \n",
    "        k1 = self.derivs(s, a)                                                                                                                                                                             \n",
    "        k2 = self.derivs(s + alpha * self.dt * k1, a)                                                                                                                                                      \n",
    "        s_1 = s + self.dt * ((1.0 - 1.0/(2.0*alpha))*k1 + (1.0/(2.0*alpha))*k2)                                                                                                                            \n",
    "        return s_1\n",
    "\n",
    "    def forward(self, o, a):\n",
    "        s_1 = self.rk2(self.inverse_trig_transform_model(o), a)\n",
    "        o_1 = torch.cat((self.trig_transform_q(s_1[:,:self.n]),s_1[:,self.n:]),1)\n",
    "        return o_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import torch\n",
    "import numpy as np\n",
    "# from models.mbrl import lnn  # Import the LNN model from the repository\n",
    "\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "\n",
    "# class LNNDynamicsWrapper:\n",
    "#     def __init__(self, env, lnn_model, device, scale_factor=1.0):\n",
    "#         self.env = env\n",
    "#         self.lnn_model = lnn_model\n",
    "#         self.device = device\n",
    "#         self.scale_factor = scale_factor\n",
    "\n",
    "#         # Handle the a_zeros parameter\n",
    "#         if env.action_space.shape[0] < env.observation_space.shape[0]:\n",
    "#             self.a_zeros = torch.zeros(\n",
    "#                 64,\n",
    "#                 env.observation_space.shape[0] - env.action_space.shape[0],\n",
    "#                 dtype=torch.float64,\n",
    "#                 device=device,\n",
    "#             )\n",
    "#         else:\n",
    "#             self.a_zeros = None\n",
    "\n",
    "#     def predict(self, state, action):\n",
    "#         \"\"\"\n",
    "#         Predict the next state and reward using the LNN model.\n",
    "#         \"\"\"\n",
    "#         state_tensor = torch.tensor(state, dtype=torch.float64, device=self.device).unsqueeze(0)\n",
    "#         action_tensor = torch.tensor(action, dtype=torch.float64, device=self.device).unsqueeze(0) * self.scale_factor\n",
    "\n",
    "#         # Handle a_zeros padding for LNN\n",
    "#         if self.a_zeros is not None:\n",
    "#             action_tensor = torch.cat([action_tensor, self.a_zeros], dim=1)\n",
    "\n",
    "#         # Predict next state using LNN\n",
    "#         with torch.no_grad():\n",
    "#             next_state = self.lnn_model(state_tensor, action_tensor)\n",
    "        \n",
    "#         next_state = next_state.squeeze(0).cpu().numpy()\n",
    "#         reward = self.env.get_reward(next_state)  # Custom reward function based on your environment\n",
    "#         return next_state, reward\n",
    "\n",
    "def get_obs(state):\n",
    "    return np.array([state[0],\n",
    "                    np.cos(state[1]),np.sin(state[1]),\n",
    "                    state[2],\n",
    "                    state[3]\n",
    "                    ])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state):\n",
    "        self.buffer.append((state, action, reward, next_state))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "        )\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def train_lnn(lnn_model, replay_buffer, optimizer, device, batch_size=64):\n",
    "    if replay_buffer.size() < batch_size:\n",
    "        return  # Skip training if not enough data\n",
    "\n",
    "    # Sample a batch of transitions\n",
    "    states, actions, _, next_states = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Convert to tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32, device=device)\n",
    "    actions = torch.tensor(actions, dtype=torch.float32, device=device).reshape((-1,1))\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Forward pass through the LNN\n",
    "    print(states.shape, actions.shape)\n",
    "    predicted_next_states = lnn_model(states, actions)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = torch.nn.functional.mse_loss(predicted_next_states, next_states)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(lnn_model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# 4. Custom Training Loop\n",
    "def train_with_lnn_and_ppo(model, lnn_model, replay_buffer, env, device, num_episodes=1000, batch_size=64):\n",
    "    lnn_optimizer = torch.optim.Adam(lnn_model.parameters(), lr=0.001)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # Collect data from the environment\n",
    "        obs, _ = env.reset()\n",
    "        print(obs.shape)\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=False)\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            # Store in replay buffer\n",
    "            replay_buffer.add(get_obs(obs), action, reward, get_obs(next_obs))\n",
    "            obs = next_obs\n",
    "            done = terminated or truncated\n",
    "\n",
    "        # Train the LNN with data from the replay buffer\n",
    "        if replay_buffer.size() >= batch_size:\n",
    "            lnn_loss = train_lnn(lnn_model, replay_buffer, lnn_optimizer, device, batch_size)\n",
    "            print(f\"Episode {episode + 1}, LNN Loss: {lnn_loss:.4f}\")\n",
    "\n",
    "        # Train the PPO agent\n",
    "        model.learn(total_timesteps=env.spec.max_episode_steps)\n",
    "\n",
    "        print(f\"Episode {episode + 1} completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1]) 2 1 5\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Starting training...\n",
      "(4,)\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.5     |\n",
      "|    ep_rew_mean     | 23.5     |\n",
      "| time/              |          |\n",
      "|    fps             | 431      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Episode 1 completed.\n",
      "(4,)\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.9     |\n",
      "|    ep_rew_mean     | 29.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 434      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Episode 2 completed.\n",
      "(4,)\n",
      "torch.Size([64, 5]) torch.Size([64, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 46\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# 5. Train PPO and LNN\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m \u001b[43mtrain_with_lnn_and_ppo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlnn_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlnn_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# 6. Evaluate the trained PPO policy\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating the trained PPO policy...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 124\u001b[0m, in \u001b[0;36mtrain_with_lnn_and_ppo\u001b[1;34m(model, lnn_model, replay_buffer, env, device, num_episodes, batch_size)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Train the LNN with data from the replay buffer\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m replay_buffer\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size:\n\u001b[1;32m--> 124\u001b[0m     lnn_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_lnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlnn_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LNN Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlnn_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Train the PPO agent\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 90\u001b[0m, in \u001b[0;36mtrain_lnn\u001b[1;34m(lnn_model, replay_buffer, optimizer, device, batch_size)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Forward pass through the LNN\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(states\u001b[38;5;241m.\u001b[39mshape, actions\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 90\u001b[0m predicted_next_states \u001b[38;5;241m=\u001b[39m \u001b[43mlnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     93\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mmse_loss(predicted_next_states, next_states)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[27], line 186\u001b[0m, in \u001b[0;36mlnn.forward\u001b[1;34m(self, o, a)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, o, a):\n\u001b[1;32m--> 186\u001b[0m     s_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrk2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_trig_transform_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m     o_1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrig_transform_q(s_1[:,:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn]),s_1[:,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn:]),\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m o_1\n",
      "Cell \u001b[1;32mIn[27], line 180\u001b[0m, in \u001b[0;36mlnn.rk2\u001b[1;34m(self, s, a)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrk2\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, a):                                                                                                                                                                                   \n\u001b[0;32m    179\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3.0\u001b[39m \u001b[38;5;66;03m# Ralston's method                                                                                                                                                                 \u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m     k1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mderivs\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m                                                                                                                                                                             \n\u001b[0;32m    181\u001b[0m     k2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mderivs(s \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m*\u001b[39m k1, a)                                                                                                                                                      \n\u001b[0;32m    182\u001b[0m     s_1 \u001b[38;5;241m=\u001b[39m s \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m*\u001b[39m ((\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2.0\u001b[39m\u001b[38;5;241m*\u001b[39malpha))\u001b[38;5;241m*\u001b[39mk1 \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2.0\u001b[39m\u001b[38;5;241m*\u001b[39malpha))\u001b[38;5;241m*\u001b[39mk2)                                                                                                                            \n",
      "Cell \u001b[1;32mIn[27], line 175\u001b[0m, in \u001b[0;36mlnn.derivs\u001b[1;34m(self, s, a)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mderivs\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, a):\n\u001b[0;32m    174\u001b[0m     q, qdot \u001b[38;5;241m=\u001b[39m s[:,:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn], s[:,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn:]\n\u001b[1;32m--> 175\u001b[0m     qddot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqdot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((qdot,qddot),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 165\u001b[0m, in \u001b[0;36mlnn.get_acc\u001b[1;34m(self, q, qdot, a)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_acc\u001b[39m(\u001b[38;5;28mself\u001b[39m, q, qdot, a):\n\u001b[0;32m    164\u001b[0m     dL_dq, L \u001b[38;5;241m=\u001b[39m jacrev(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_L, has_aux\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(q)\n\u001b[1;32m--> 165\u001b[0m     term_1 \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mblk,bijk->bijl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdL_dq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     dM_dq \u001b[38;5;241m=\u001b[39m term_1 \u001b[38;5;241m+\u001b[39m term_1\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    167\u001b[0m     c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbjik,bk,bj->bi\u001b[39m\u001b[38;5;124m'\u001b[39m, dM_dq, qdot, qdot) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbikj,bk,bj->bi\u001b[39m\u001b[38;5;124m'\u001b[39m, dM_dq, qdot, qdot)        \n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\functional.py:402\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    404\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "# 1. Initialize the environment\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "obs_size = 5\n",
    "action_size = 1  # Continuous action for CartPole (custom implementation)\n",
    "n =2\n",
    "batch_size= 64\n",
    "# 2. Initialize the LNN model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lnn_model = lnn(\n",
    "    env_name=\"cartpole\",\n",
    "    n=n,  # Number of generalized coordinates (cart position, velocity, angle, angular velocity)\n",
    "    obs_size=obs_size,\n",
    "    action_size=action_size,\n",
    "    dt=0.02,  # Time step\n",
    "    a_zeros = torch.zeros(\n",
    "        batch_size, max(0, n - action_size), dtype=torch.float32, device=device\n",
    "    ) if action_size < n else None\n",
    ").to(device)\n",
    "\n",
    "# 3. Initialize PPO agent\n",
    "model = PPO(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=0.0003,\n",
    "    n_steps=2048,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.0,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# 4. Initialize replay buffer for LNN training\n",
    "replay_buffer = ReplayBuffer(max_size=10000)\n",
    "\n",
    "# 5. Train PPO and LNN\n",
    "print(\"Starting training...\")\n",
    "train_with_lnn_and_ppo(\n",
    "    model=model,\n",
    "    lnn_model=lnn_model,\n",
    "    replay_buffer=replay_buffer,\n",
    "    env=env,\n",
    "    device=device,\n",
    "    num_episodes=1000,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "# 6. Evaluate the trained PPO policy\n",
    "print(\"Evaluating the trained PPO policy...\")\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# 7. Save the trained PPO model and LNN\n",
    "print(\"Saving the models...\")\n",
    "model.save(\"ppo_cartpole_with_lnn\")\n",
    "torch.save(lnn_model.state_dict(), \"lnn_cartpole.pth\")\n",
    "\n",
    "# 8. Visualize the PPO agent\n",
    "print(\"Running the trained PPO policy...\")\n",
    "obs, _ = env.reset()\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, _, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
