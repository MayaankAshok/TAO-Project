{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mayaank\\AppData\\Local\\Temp\\ipykernel_17648\\403230884.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(self.state, dtype=torch.float32, device=self.device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 306  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 332         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001371671 |\n",
      "|    clip_fraction        | 0.00708     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.44       |\n",
      "|    explained_variance   | -0.229      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00876    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00154    |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 0.0491      |\n",
      "-----------------------------------------\n",
      "Episode: 2, LNN loss: 1.036028265953064, Rewards Loss: 6.464569091796875\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 291          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039408747 |\n",
      "|    clip_fraction        | 0.0238       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.209       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00923     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.0149       |\n",
      "------------------------------------------\n",
      "Episode: 3, LNN loss: 0.8246325254440308, Rewards Loss: 5.75775146484375\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 310          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036869114 |\n",
      "|    clip_fraction        | 0.0488       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -0.397       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00825     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00285     |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 0.00775      |\n",
      "------------------------------------------\n",
      "Episode: 4, LNN loss: 0.8434256911277771, Rewards Loss: 5.786402225494385\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 287          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055868244 |\n",
      "|    clip_fraction        | 0.0486       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -0.307       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00571      |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00265     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.00441      |\n",
      "------------------------------------------\n",
      "Episode: 5, LNN loss: 0.7672594785690308, Rewards Loss: 4.8569841384887695\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 298          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073822397 |\n",
      "|    clip_fraction        | 0.0849       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.294       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0236      |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00559     |\n",
      "|    std                  | 0.99         |\n",
      "|    value_loss           | 0.00277      |\n",
      "------------------------------------------\n",
      "Episode: 6, LNN loss: 0.7757565379142761, Rewards Loss: 5.902046203613281\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 293         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006553406 |\n",
      "|    clip_fraction        | 0.0805      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.4        |\n",
      "|    explained_variance   | -0.363      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00729     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00427    |\n",
      "|    std                  | 0.976       |\n",
      "|    value_loss           | 0.00175     |\n",
      "-----------------------------------------\n",
      "Episode: 7, LNN loss: 0.5929377675056458, Rewards Loss: 5.9661970138549805\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 284         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007966975 |\n",
      "|    clip_fraction        | 0.0847      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | -0.48       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.029      |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00783    |\n",
      "|    std                  | 0.952       |\n",
      "|    value_loss           | 0.00122     |\n",
      "-----------------------------------------\n",
      "Episode: 8, LNN loss: 0.7290178537368774, Rewards Loss: 5.615069389343262\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 294         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008073131 |\n",
      "|    clip_fraction        | 0.0915      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.36       |\n",
      "|    explained_variance   | -0.534      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0119     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00514    |\n",
      "|    std                  | 0.931       |\n",
      "|    value_loss           | 0.00076     |\n",
      "-----------------------------------------\n",
      "Episode: 9, LNN loss: 0.9226657152175903, Rewards Loss: 7.01802921295166\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 307         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010555478 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | -0.802      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0235     |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00782    |\n",
      "|    std                  | 0.9         |\n",
      "|    value_loss           | 0.000548    |\n",
      "-----------------------------------------\n",
      "Episode: 10, LNN loss: 0.8771483302116394, Rewards Loss: 6.6220703125\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 305         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009417515 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | -0.753      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0113     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.01       |\n",
      "|    std                  | 0.87        |\n",
      "|    value_loss           | 0.000474    |\n",
      "-----------------------------------------\n",
      "Episode: 11, LNN loss: 1.0931624174118042, Rewards Loss: 7.124151229858398\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 314         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012834933 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | -0.796      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0339     |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    std                  | 0.837       |\n",
      "|    value_loss           | 0.000273    |\n",
      "-----------------------------------------\n",
      "Episode: 12, LNN loss: 1.423154354095459, Rewards Loss: 7.993471145629883\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 318        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01230057 |\n",
      "|    clip_fraction        | 0.139      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.22      |\n",
      "|    explained_variance   | -0.817     |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00985   |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.0126    |\n",
      "|    std                  | 0.808      |\n",
      "|    value_loss           | 0.000162   |\n",
      "----------------------------------------\n",
      "Episode: 13, LNN loss: 1.327264666557312, Rewards Loss: 7.704610347747803\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 316          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046394793 |\n",
      "|    clip_fraction        | 0.0736       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.18        |\n",
      "|    explained_variance   | -0.871       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0296      |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00856     |\n",
      "|    std                  | 0.771        |\n",
      "|    value_loss           | 0.000143     |\n",
      "------------------------------------------\n",
      "Episode: 14, LNN loss: 1.0626273155212402, Rewards Loss: 6.933699607849121\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 312         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005626498 |\n",
      "|    clip_fraction        | 0.0464      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | -0.701      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.000224    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00917    |\n",
      "|    std                  | 0.723       |\n",
      "|    value_loss           | 8.36e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 15, LNN loss: 1.0479978322982788, Rewards Loss: 7.051469802856445\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 318          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043599717 |\n",
      "|    clip_fraction        | 0.0456       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | -0.568       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0342      |\n",
      "|    n_updates            | 150          |\n",
      "|    policy_gradient_loss | -0.00599     |\n",
      "|    std                  | 0.693        |\n",
      "|    value_loss           | 4.59e-05     |\n",
      "------------------------------------------\n",
      "Episode: 16, LNN loss: 0.8719624876976013, Rewards Loss: 6.362037658691406\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 315         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005369715 |\n",
      "|    clip_fraction        | 0.0407      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | -0.802      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00622    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00871    |\n",
      "|    std                  | 0.652       |\n",
      "|    value_loss           | 3.38e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 17, LNN loss: 0.8681315183639526, Rewards Loss: 6.178528785705566\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 316          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 36864        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0030923202 |\n",
      "|    clip_fraction        | 0.048        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.967       |\n",
      "|    explained_variance   | -0.973       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0128      |\n",
      "|    n_updates            | 170          |\n",
      "|    policy_gradient_loss | -0.00708     |\n",
      "|    std                  | 0.625        |\n",
      "|    value_loss           | 2.67e-05     |\n",
      "------------------------------------------\n",
      "Episode: 18, LNN loss: 0.8719798922538757, Rewards Loss: 5.987490177154541\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 318         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007294493 |\n",
      "|    clip_fraction        | 0.0369      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.911      |\n",
      "|    explained_variance   | -0.781      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00619     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.00844    |\n",
      "|    std                  | 0.584       |\n",
      "|    value_loss           | 1.28e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 19, LNN loss: 0.9156880378723145, Rewards Loss: 5.333432197570801\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 317         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009171272 |\n",
      "|    clip_fraction        | 0.0604      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.839      |\n",
      "|    explained_variance   | -0.706      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0102     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0089     |\n",
      "|    std                  | 0.541       |\n",
      "|    value_loss           | 8.47e-06    |\n",
      "-----------------------------------------\n",
      "Episode: 20, LNN loss: 0.8818669319152832, Rewards Loss: 5.564872741699219\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 319         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010606023 |\n",
      "|    clip_fraction        | 0.0359      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.765      |\n",
      "|    explained_variance   | -0.63       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0174     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00852    |\n",
      "|    std                  | 0.504       |\n",
      "|    value_loss           | 7.55e-06    |\n",
      "-----------------------------------------\n",
      "Episode: 21, LNN loss: 0.7639031410217285, Rewards Loss: 6.433794975280762\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 313          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0090664765 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.694       |\n",
      "|    explained_variance   | -0.652       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00715     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00804     |\n",
      "|    std                  | 0.469        |\n",
      "|    value_loss           | 9.84e-06     |\n",
      "------------------------------------------\n",
      "Episode: 22, LNN loss: 0.530009388923645, Rewards Loss: 5.809743881225586\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 314         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005945289 |\n",
      "|    clip_fraction        | 0.04        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.625      |\n",
      "|    explained_variance   | -0.468      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0198     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00841    |\n",
      "|    std                  | 0.437       |\n",
      "|    value_loss           | 2.03e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 23, LNN loss: 0.545039176940918, Rewards Loss: 6.50211763381958\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 316         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009391805 |\n",
      "|    clip_fraction        | 0.0369      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.548      |\n",
      "|    explained_variance   | -0.364      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0204     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    std                  | 0.404       |\n",
      "|    value_loss           | 1.21e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 24, LNN loss: 0.6545935869216919, Rewards Loss: 6.5698561668396\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 308         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009815323 |\n",
      "|    clip_fraction        | 0.0352      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.474      |\n",
      "|    explained_variance   | -0.304      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0223     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00913    |\n",
      "|    std                  | 0.376       |\n",
      "|    value_loss           | 3.74e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 25, LNN loss: 0.9470992088317871, Rewards Loss: 7.0405073165893555\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 309          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059903236 |\n",
      "|    clip_fraction        | 0.026        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.411       |\n",
      "|    explained_variance   | -0.359       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0139      |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00577     |\n",
      "|    std                  | 0.355        |\n",
      "|    value_loss           | 0.000124     |\n",
      "------------------------------------------\n",
      "Episode: 26, LNN loss: 1.282752275466919, Rewards Loss: 7.31417179107666\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 318         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009694746 |\n",
      "|    clip_fraction        | 0.0573      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.346      |\n",
      "|    explained_variance   | -0.148      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00731    |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    std                  | 0.331       |\n",
      "|    value_loss           | 1.25e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 27, LNN loss: 1.253488302230835, Rewards Loss: 6.669534683227539\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 314          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0077631716 |\n",
      "|    clip_fraction        | 0.0436       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.274       |\n",
      "|    explained_variance   | -0.122       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0237      |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.0102      |\n",
      "|    std                  | 0.307        |\n",
      "|    value_loss           | 5.11e-05     |\n",
      "------------------------------------------\n",
      "Episode: 28, LNN loss: 1.107521414756775, Rewards Loss: 6.281983375549316\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 310          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066544497 |\n",
      "|    clip_fraction        | 0.0408       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.202       |\n",
      "|    explained_variance   | -0.11        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0232      |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00844     |\n",
      "|    std                  | 0.288        |\n",
      "|    value_loss           | 1.66e-05     |\n",
      "------------------------------------------\n",
      "Episode: 29, LNN loss: 0.8686735033988953, Rewards Loss: 5.544833183288574\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 320          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035332358 |\n",
      "|    clip_fraction        | 0.0271       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.143       |\n",
      "|    explained_variance   | -0.106       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0044      |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00607     |\n",
      "|    std                  | 0.272        |\n",
      "|    value_loss           | 4.45e-05     |\n",
      "------------------------------------------\n",
      "Episode: 30, LNN loss: 0.9141442775726318, Rewards Loss: 5.200683116912842\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 316         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009819698 |\n",
      "|    clip_fraction        | 0.0358      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.0814     |\n",
      "|    explained_variance   | -0.0469     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00589    |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00845    |\n",
      "|    std                  | 0.255       |\n",
      "|    value_loss           | 2.37e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 31, LNN loss: 0.8151146769523621, Rewards Loss: 5.2807512283325195\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 318          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063721454 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.0191      |\n",
      "|    explained_variance   | -0.0525      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0254      |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00729     |\n",
      "|    std                  | 0.239        |\n",
      "|    value_loss           | 0.000461     |\n",
      "------------------------------------------\n",
      "Episode: 32, LNN loss: 0.9461830854415894, Rewards Loss: 6.156099319458008\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 320          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045120856 |\n",
      "|    clip_fraction        | 0.0439       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.0418       |\n",
      "|    explained_variance   | -0.0403      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00616     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00613     |\n",
      "|    std                  | 0.226        |\n",
      "|    value_loss           | 1.84e-05     |\n",
      "------------------------------------------\n",
      "Episode: 33, LNN loss: 0.8053860664367676, Rewards Loss: 5.40093994140625\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 320          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075305346 |\n",
      "|    clip_fraction        | 0.076        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.0994       |\n",
      "|    explained_variance   | -0.0368      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00709      |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00639     |\n",
      "|    std                  | 0.213        |\n",
      "|    value_loss           | 4.48e-05     |\n",
      "------------------------------------------\n",
      "Episode: 34, LNN loss: 0.8922194242477417, Rewards Loss: 6.420285224914551\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 315         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003992709 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.156       |\n",
      "|    explained_variance   | -0.0264     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00364     |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00417    |\n",
      "|    std                  | 0.202       |\n",
      "|    value_loss           | 8.94e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 35, LNN loss: 0.6611107587814331, Rewards Loss: 6.853304862976074\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 318          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071327547 |\n",
      "|    clip_fraction        | 0.035        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | 0.218        |\n",
      "|    explained_variance   | -0.0197      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.017        |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00698     |\n",
      "|    std                  | 0.189        |\n",
      "|    value_loss           | 1.01e-05     |\n",
      "------------------------------------------\n",
      "Episode: 36, LNN loss: 0.5261275768280029, Rewards Loss: 5.690991401672363\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 321         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008589685 |\n",
      "|    clip_fraction        | 0.0648      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.278       |\n",
      "|    explained_variance   | -0.0156     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00501     |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00507    |\n",
      "|    std                  | 0.178       |\n",
      "|    value_loss           | 9.92e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 37, LNN loss: 0.38435810804367065, Rewards Loss: 5.325407981872559\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 317         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009591127 |\n",
      "|    clip_fraction        | 0.0636      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.33        |\n",
      "|    explained_variance   | -0.0112     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0249     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00717    |\n",
      "|    std                  | 0.17        |\n",
      "|    value_loss           | 4.76e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 38, LNN loss: 0.5053432583808899, Rewards Loss: 4.883888244628906\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 316         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007926125 |\n",
      "|    clip_fraction        | 0.0917      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.373       |\n",
      "|    explained_variance   | -0.0151     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0141      |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00233    |\n",
      "|    std                  | 0.164       |\n",
      "|    value_loss           | 1.43e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 39, LNN loss: 0.7880260348320007, Rewards Loss: 5.759274482727051\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 320         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005533048 |\n",
      "|    clip_fraction        | 0.0666      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.416       |\n",
      "|    explained_variance   | -0.00294    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0203      |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00455    |\n",
      "|    std                  | 0.155       |\n",
      "|    value_loss           | 9.5e-05     |\n",
      "-----------------------------------------\n",
      "Episode: 40, LNN loss: 1.0259424448013306, Rewards Loss: 5.371218681335449\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 317         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009443585 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.466       |\n",
      "|    explained_variance   | -0.00287    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0285     |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00509    |\n",
      "|    std                  | 0.148       |\n",
      "|    value_loss           | 0.000117    |\n",
      "-----------------------------------------\n",
      "Episode: 41, LNN loss: 1.0159039497375488, Rewards Loss: 5.524142265319824\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 290        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00393884 |\n",
      "|    clip_fraction        | 0.0714     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.514      |\n",
      "|    explained_variance   | -0.0024    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0123     |\n",
      "|    n_updates            | 410        |\n",
      "|    policy_gradient_loss | -0.00242   |\n",
      "|    std                  | 0.142      |\n",
      "|    value_loss           | 3.19e-05   |\n",
      "----------------------------------------\n",
      "Episode: 42, LNN loss: 1.1288666725158691, Rewards Loss: 5.696344375610352\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 309         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007986128 |\n",
      "|    clip_fraction        | 0.0958      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.563       |\n",
      "|    explained_variance   | -0.00947    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0118     |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00052    |\n",
      "|    std                  | 0.135       |\n",
      "|    value_loss           | 2.54e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 43, LNN loss: 1.0996874570846558, Rewards Loss: 5.306785583496094\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 308         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004566853 |\n",
      "|    clip_fraction        | 0.0626      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.616       |\n",
      "|    explained_variance   | -0.017      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.01       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00255    |\n",
      "|    std                  | 0.127       |\n",
      "|    value_loss           | 7.91e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 44, LNN loss: 0.9029874801635742, Rewards Loss: 5.577103614807129\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 306         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012607622 |\n",
      "|    clip_fraction        | 0.0809      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.671       |\n",
      "|    explained_variance   | -0.00435    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00475    |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0013     |\n",
      "|    std                  | 0.121       |\n",
      "|    value_loss           | 3.82e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 45, LNN loss: 0.8652216792106628, Rewards Loss: 5.110002517700195\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 311         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003991651 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.716       |\n",
      "|    explained_variance   | -0.00731    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0227     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.000325   |\n",
      "|    std                  | 0.116       |\n",
      "|    value_loss           | 6.63e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 46, LNN loss: 0.7380262613296509, Rewards Loss: 5.280558109283447\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 308        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01286675 |\n",
      "|    clip_fraction        | 0.14       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.757      |\n",
      "|    explained_variance   | -0.00781   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0333     |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | 0.00118    |\n",
      "|    std                  | 0.111      |\n",
      "|    value_loss           | 0.000907   |\n",
      "----------------------------------------\n",
      "Episode: 47, LNN loss: 0.8733730316162109, Rewards Loss: 6.098597526550293\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 310         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004814426 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.79        |\n",
      "|    explained_variance   | -0.133      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0183     |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 0.00263     |\n",
      "|    std                  | 0.109       |\n",
      "|    value_loss           | 4.42e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 48, LNN loss: 0.8743049502372742, Rewards Loss: 6.274267196655273\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 278         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014148237 |\n",
      "|    clip_fraction        | 0.0702      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | 0.834       |\n",
      "|    explained_variance   | -0.0186     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00723    |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00365    |\n",
      "|    std                  | 0.102       |\n",
      "|    value_loss           | 8.48e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 49, LNN loss: 1.1294512748718262, Rewards Loss: 6.457795143127441\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 290        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01016674 |\n",
      "|    clip_fraction        | 0.109      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | 0.891      |\n",
      "|    explained_variance   | -0.00669   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.008     |\n",
      "|    n_updates            | 490        |\n",
      "|    policy_gradient_loss | 0.000555   |\n",
      "|    std                  | 0.0972     |\n",
      "|    value_loss           | 7.08e-05   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 211\u001b[0m\n\u001b[0;32m    208\u001b[0m model\u001b[38;5;241m.\u001b[39mset_logger(new_logger)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m trained_model, trained_lnn, trained_reward \u001b[38;5;241m=\u001b[39m train_with_lnn_and_ppo(\n\u001b[0;32m    212\u001b[0m     model,  env, custom_env, device, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m\n\u001b[0;32m    213\u001b[0m )\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Save models\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;66;03m# torch.save(custom_env.lnn_model.state_dict(), \"lnn_cartpole.pth\")\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# torch.save(custom_env.reward_model.state_dict(), \"reward_cartpole.pth\")\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;66;03m# trained_model.save(\"ppo_cartpole_with_lnn\")\u001b[39;00m\n\u001b[0;32m    219\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_cartpole_with_lnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class LNNModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(LNNModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, state_dim)  # Predicts next state\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # print(\">>>>\", state.shape, action.shape)\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class RewardNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=256):\n",
    "        super(RewardNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Predicts reward\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.float32),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.tensor(next_states, dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class CustomGymEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gym environment that uses LNN for state transitions, Reward Network for reward calculations,\n",
    "    and incorporates a replay buffer.\n",
    "    \"\"\"\n",
    "    def __init__(self, lnn_model, reward_model, state_dim, action_dim, action_space, observation_space, device, replay_buffer):\n",
    "        super(CustomGymEnvironment, self).__init__()\n",
    "        self.lnn_model = lnn_model\n",
    "        self.reward_model = reward_model\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        # Define observation and action space\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # Initialize state\n",
    "        self.state = np.zeros(state_dim)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # print(\"<<<\", self.state.shape)\n",
    "        if len(self.replay_buffer) > 0:\n",
    "            # Sample a random state from the replay buffer\n",
    "            sample = self.replay_buffer.sample(1)\n",
    "            # print(sample.shape)\n",
    "            if sample:\n",
    "                self.state = sample[0][0]\n",
    "        else:\n",
    "            # Otherwise, initialize randomly\n",
    "            self.state = self.observation_space.sample()\n",
    "            # print(self.state.shape)\n",
    "        # print(\"<<<2\", self.state.shape)\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Convert state and action to tensors\n",
    "        # print(action.shape)\n",
    "        state_tensor = torch.tensor(self.state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        action_tensor = torch.tensor(action, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        # print(\"Hii\",state_tensor.shape, action_tensor.shape)\n",
    "        # Use LNN to predict the next state\n",
    "        next_state = self.lnn_model(state_tensor, action_tensor).squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "        # Use Reward Network to calculate reward\n",
    "        reward = (\n",
    "            self.reward_model(\n",
    "                torch.tensor(next_state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            )\n",
    "            .item()\n",
    "        )\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = np.abs(self.state[0])  <= 0.1 and np.abs(self.state[1]) <= 0.01   # Example condition\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        self.replay_buffer.add(self.state, action, reward, next_state, done)\n",
    "\n",
    "        # Update state\n",
    "        self.state = next_state\n",
    "\n",
    "        return next_state, reward, done, False, {}\n",
    "\n",
    "\n",
    "def train_with_lnn_and_ppo(\n",
    "    model, env, custom_env, device, num_episodes, batch_size\n",
    "):\n",
    "    reward_optimizer = optim.Adam(reward_network.parameters(), lr=2e-6)\n",
    "    lnn_optimizer = optim.Adam(custom_env.lnn_model.parameters(), lr=2e-6)\n",
    "    reward_loss_fn = nn.MSELoss()\n",
    "    lnn_loss_fn = nn.MSELoss()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Collect transitions using PPO policy\n",
    "            action, _ = model.predict(state, deterministic=False)\n",
    "            state1, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # print(state.shape, action.shape)\n",
    "            custom_env.replay_buffer.add(state, action, reward, state1, done)\n",
    "\n",
    "\n",
    "        # Train LNN and Reward Network\n",
    "        if len(custom_env.replay_buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = custom_env.replay_buffer.sample(batch_size)\n",
    "\n",
    "            # Train LNN\n",
    "            lnn_preds = custom_env.lnn_model(states.to(device), actions.to(device))\n",
    "            lnn_loss = lnn_loss_fn(lnn_preds, next_states.to(device))\n",
    "            lnn_optimizer.zero_grad()\n",
    "            lnn_loss.backward()\n",
    "            lnn_optimizer.step()\n",
    "\n",
    "            # Train Reward Network\n",
    "            reward_preds = custom_env.reward_model(next_states.to(device))\n",
    "            reward_loss = reward_loss_fn(reward_preds, rewards.to(device).unsqueeze(-1))\n",
    "            reward_optimizer.zero_grad()\n",
    "            reward_loss.backward()\n",
    "            reward_optimizer.step()\n",
    "\n",
    "            print(f\"Episode: {episode}, LNN loss: {lnn_loss}, Rewards Loss: {reward_loss}\")\n",
    "        # Train PPO policy\n",
    "        model.learn(total_timesteps=2048, reset_num_timesteps=False)\n",
    "\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "tmp_path = \"./\"\n",
    "# set up logger\n",
    "new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "\n",
    "\n",
    "# Setup\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "# print(\"Action Dim\", action_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lnn_model = LNNModel(state_dim, action_dim).to(device)\n",
    "reward_network = RewardNetwork(state_dim).to(device)\n",
    "replay_buffer = ReplayBuffer(max_size=10000)\n",
    "\n",
    "action_space = env.action_space\n",
    "\n",
    "custom_env = CustomGymEnvironment(\n",
    "    lnn_model=lnn_model,\n",
    "    reward_model=reward_network,\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    action_space=action_space,\n",
    "    observation_space=env.observation_space,\n",
    "    device=device,\n",
    "    replay_buffer=replay_buffer,\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", custom_env, verbose=1)\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Train\n",
    "trained_model, trained_lnn, trained_reward = train_with_lnn_and_ppo(\n",
    "    model,  env, custom_env, device, num_episodes=50, batch_size=4096\n",
    ")\n",
    "\n",
    "# Save models\n",
    "# torch.save(custom_env.lnn_model.state_dict(), \"lnn_cartpole.pth\")\n",
    "# torch.save(custom_env.reward_model.state_dict(), \"reward_cartpole.pth\")\n",
    "# trained_model.save(\"ppo_cartpole_with_lnn\")\n",
    "model.save(\"ppo_cartpole_with_lnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_cartpole_with_lnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -1080.87 +/- 308.68\n",
      "Done  199\n",
      "Done  399\n",
      "Done  599\n",
      "Done  799\n",
      "Done  999\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\", render_mode = 'human')\n",
    "# 5. Load the trained model (optional)\n",
    "model = PPO.load(\"ppo_cartpole_with_lnn\", env=env)\n",
    "\n",
    "# 6. Evaluate the trained policy\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=3)\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# 7. Run the trained agent\n",
    "obs, _ = env.reset()\n",
    "for i in range(1000):  # Run for a fixed number of timesteps\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "        print(\"Done \", i)\n",
    "    # if not i%20: print(i)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
