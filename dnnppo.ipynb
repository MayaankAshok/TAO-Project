{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from icecream import ic\n",
    "\n",
    "#! Set the environment variables to override gpu (specically for my device)\n",
    "os.environ['HSA_OVERRIDE_GFX_VERSION'] = '10.3.0'\n",
    "os.environ['ROCBLAS_TENSILE_LIBRARY'] = '/home/autrio/.local/lib/python3.10/site-packages/torch/lib/rocblas/library/TensileLibrary_lazy_gfx1030.dat'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6400/403230884.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(self.state, dtype=torch.float32, device=self.device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 803  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 687          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049869497 |\n",
      "|    clip_fraction        | 0.0277       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.256       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00564      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0025      |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 0.00648      |\n",
      "------------------------------------------\n",
      "Episode: 2, LNN loss: 0.502531886100769, Rewards Loss: 6.645652770996094\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 801          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043779593 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -0.116       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0206      |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00286     |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 0.0026       |\n",
      "------------------------------------------\n",
      "Episode: 3, LNN loss: 0.5516272783279419, Rewards Loss: 5.594508171081543\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 757          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0017530447 |\n",
      "|    clip_fraction        | 0.000537     |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -0.362       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00881     |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -4.4e-05     |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 0.00159      |\n",
      "------------------------------------------\n",
      "Episode: 4, LNN loss: 0.4810759425163269, Rewards Loss: 5.624563217163086\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 740         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002215838 |\n",
      "|    clip_fraction        | 0.0108      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | -0.815      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00298    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00105    |\n",
      "|    std                  | 0.992       |\n",
      "|    value_loss           | 0.000952    |\n",
      "-----------------------------------------\n",
      "Episode: 5, LNN loss: 0.4482618272304535, Rewards Loss: 4.466544151306152\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 747          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041027283 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | -1.22        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0102      |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.00215     |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 0.000653     |\n",
      "------------------------------------------\n",
      "Episode: 6, LNN loss: 0.5249367952346802, Rewards Loss: 4.602015495300293\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 790           |\n",
      "|    iterations           | 1             |\n",
      "|    time_elapsed         | 2             |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00016477917 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.42         |\n",
      "|    explained_variance   | -1.65         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 0.00255       |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | 0.000257      |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 0.000459      |\n",
      "-------------------------------------------\n",
      "Episode: 7, LNN loss: 0.5504911541938782, Rewards Loss: 4.432300567626953\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 785          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022304782 |\n",
      "|    clip_fraction        | 0.00654      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -2.31        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00426      |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.000784    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.000319     |\n",
      "------------------------------------------\n",
      "Episode: 8, LNN loss: 0.512560248374939, Rewards Loss: 4.376829147338867\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 793         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004776489 |\n",
      "|    clip_fraction        | 0.0269      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -4.33       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.000812   |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00267    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 0.000229    |\n",
      "-----------------------------------------\n",
      "Episode: 9, LNN loss: 0.548672080039978, Rewards Loss: 4.56108283996582\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 804          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034991722 |\n",
      "|    clip_fraction        | 0.00781      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -7.06        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0161      |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000451    |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 0.000165     |\n",
      "------------------------------------------\n",
      "Episode: 10, LNN loss: 0.5752652287483215, Rewards Loss: 4.090025901794434\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 705          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015377931 |\n",
      "|    clip_fraction        | 0.00264      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -6.05        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00359      |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.000257    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 0.000126     |\n",
      "------------------------------------------\n",
      "Episode: 11, LNN loss: 0.4878111779689789, Rewards Loss: 4.53400993347168\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 738          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036399188 |\n",
      "|    clip_fraction        | 0.00664      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -11.7        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0112       |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.000646    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.64e-05     |\n",
      "------------------------------------------\n",
      "Episode: 12, LNN loss: 0.41652271151542664, Rewards Loss: 4.399297714233398\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 791          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043557193 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | -7.15        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00978      |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00218     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 6.88e-05     |\n",
      "------------------------------------------\n",
      "Episode: 13, LNN loss: 0.3518699109554291, Rewards Loss: 5.451991081237793\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 735          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006205662 |\n",
      "|    clip_fraction        | 0.0153       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | -6.91        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00251     |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.000754    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.59e-05     |\n",
      "------------------------------------------\n",
      "Episode: 14, LNN loss: 0.41964516043663025, Rewards Loss: 5.093131065368652\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 762          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0021222343 |\n",
      "|    clip_fraction        | 0.015        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.44        |\n",
      "|    explained_variance   | -5.1         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00754      |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    std                  | 1.03         |\n",
      "|    value_loss           | 3.37e-05     |\n",
      "------------------------------------------\n",
      "Episode: 15, LNN loss: 0.3333098292350769, Rewards Loss: 5.775988578796387\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 759        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 2          |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00530584 |\n",
      "|    clip_fraction        | 0.0393     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.45      |\n",
      "|    explained_variance   | -3.42      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00257   |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | -0.00384   |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 2.46e-05   |\n",
      "----------------------------------------\n",
      "Episode: 16, LNN loss: 0.3570166230201721, Rewards Loss: 5.717047691345215\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 776         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007844899 |\n",
      "|    clip_fraction        | 0.0864      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | -2.18       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0111     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 1.91e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 17, LNN loss: 0.36488232016563416, Rewards Loss: 5.789501667022705\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 770        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 2          |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00870463 |\n",
      "|    clip_fraction        | 0.0917     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | -1.22      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.0109    |\n",
      "|    n_updates            | 170        |\n",
      "|    policy_gradient_loss | -0.0109    |\n",
      "|    std                  | 1.08       |\n",
      "|    value_loss           | 1.81e-05   |\n",
      "----------------------------------------\n",
      "Episode: 18, LNN loss: 0.3078499138355255, Rewards Loss: 5.359137058258057\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 714         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010443231 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.51       |\n",
      "|    explained_variance   | -0.738      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.015       |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0145     |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 1.72e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 19, LNN loss: 0.36466607451438904, Rewards Loss: 6.032695770263672\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 748         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015050355 |\n",
      "|    clip_fraction        | 0.181       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.55       |\n",
      "|    explained_variance   | -0.336      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.048      |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0213     |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 2.16e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 20, LNN loss: 0.37596607208251953, Rewards Loss: 5.910391807556152\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 676         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019717213 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | -0.163      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0272     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    std                  | 1.19        |\n",
      "|    value_loss           | 2.04e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 21, LNN loss: 0.27368518710136414, Rewards Loss: 5.911009788513184\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 732         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020725673 |\n",
      "|    clip_fraction        | 0.287       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.59       |\n",
      "|    explained_variance   | -0.0682     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.032      |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0277     |\n",
      "|    std                  | 1.18        |\n",
      "|    value_loss           | 1.6e-05     |\n",
      "-----------------------------------------\n",
      "Episode: 22, LNN loss: 0.29276925325393677, Rewards Loss: 6.1455888748168945\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020341069 |\n",
      "|    clip_fraction        | 0.299       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.57       |\n",
      "|    explained_variance   | -0.0292     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0255     |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0324     |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 1.96e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 23, LNN loss: 0.25610780715942383, Rewards Loss: 5.021021366119385\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 789         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020695055 |\n",
      "|    clip_fraction        | 0.293       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | -0.0168     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0236     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0353     |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 2.08e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 24, LNN loss: 0.10684320330619812, Rewards Loss: 5.625863075256348\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 781         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019504745 |\n",
      "|    clip_fraction        | 0.261       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | -0.0182     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0305     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0308     |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 1.86e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 25, LNN loss: 0.11963596194982529, Rewards Loss: 6.197378158569336\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 632        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01988506 |\n",
      "|    clip_fraction        | 0.224      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.41      |\n",
      "|    explained_variance   | -0.00337   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | -0.00932   |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0286    |\n",
      "|    std                  | 0.964      |\n",
      "|    value_loss           | 1.74e-05   |\n",
      "----------------------------------------\n",
      "Episode: 26, LNN loss: 0.12772725522518158, Rewards Loss: 5.979310989379883\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 834         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022598341 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | -0.00524    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0568     |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0306     |\n",
      "|    std                  | 0.897       |\n",
      "|    value_loss           | 1.99e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 27, LNN loss: 0.21663598716259003, Rewards Loss: 5.401595592498779\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 842         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011334313 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | -0.00121    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0365     |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.0195     |\n",
      "|    std                  | 0.851       |\n",
      "|    value_loss           | 1.14e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 28, LNN loss: 0.2078123390674591, Rewards Loss: 5.552350997924805\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 858         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009886552 |\n",
      "|    clip_fraction        | 0.107       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.23       |\n",
      "|    explained_variance   | -0.00195    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0132     |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    std                  | 0.817       |\n",
      "|    value_loss           | 2.02e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 29, LNN loss: 0.2593480348587036, Rewards Loss: 5.422647476196289\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 825          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070716618 |\n",
      "|    clip_fraction        | 0.072        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.19        |\n",
      "|    explained_variance   | -0.000184    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0155      |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.0111      |\n",
      "|    std                  | 0.778        |\n",
      "|    value_loss           | 1.06e-05     |\n",
      "------------------------------------------\n",
      "Episode: 30, LNN loss: 0.21987955272197723, Rewards Loss: 5.800421237945557\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 667         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004297455 |\n",
      "|    clip_fraction        | 0.0721      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | -8.86e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0331     |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    std                  | 0.744       |\n",
      "|    value_loss           | 1.96e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 31, LNN loss: 0.20589470863342285, Rewards Loss: 4.930775165557861\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 779         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002970891 |\n",
      "|    clip_fraction        | 0.0357      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | -0.000354   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00323    |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00493    |\n",
      "|    std                  | 0.714       |\n",
      "|    value_loss           | 4.3e-05     |\n",
      "-----------------------------------------\n",
      "Episode: 32, LNN loss: 0.17831239104270935, Rewards Loss: 5.004315376281738\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 686          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065895747 |\n",
      "|    clip_fraction        | 0.0737       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.06        |\n",
      "|    explained_variance   | 0.000273     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00154      |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00901     |\n",
      "|    std                  | 0.693        |\n",
      "|    value_loss           | 3.77e-05     |\n",
      "------------------------------------------\n",
      "Episode: 33, LNN loss: 0.14586631953716278, Rewards Loss: 5.352002143859863\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 668       |\n",
      "|    iterations           | 1         |\n",
      "|    time_elapsed         | 3         |\n",
      "|    total_timesteps      | 69632     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0046622 |\n",
      "|    clip_fraction        | 0.0359    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.02     |\n",
      "|    explained_variance   | -0.00225  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | -0.00114  |\n",
      "|    n_updates            | 330       |\n",
      "|    policy_gradient_loss | -0.00375  |\n",
      "|    std                  | 0.657     |\n",
      "|    value_loss           | 5.21e-05  |\n",
      "---------------------------------------\n",
      "Episode: 34, LNN loss: 0.29772019386291504, Rewards Loss: 5.207949161529541\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 821          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0015378816 |\n",
      "|    clip_fraction        | 0.0191       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.979       |\n",
      "|    explained_variance   | -0.00073     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.00112      |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    std                  | 0.634        |\n",
      "|    value_loss           | 1.47e-05     |\n",
      "------------------------------------------\n",
      "Episode: 35, LNN loss: 0.5908620357513428, Rewards Loss: 5.255441665649414\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 736         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004017475 |\n",
      "|    clip_fraction        | 0.0192      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.943      |\n",
      "|    explained_variance   | -0.0016     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00401    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.000192   |\n",
      "|    std                  | 0.611       |\n",
      "|    value_loss           | 3.37e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 36, LNN loss: 0.6631245613098145, Rewards Loss: 5.705338001251221\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 792          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035524643 |\n",
      "|    clip_fraction        | 0.0587       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.911       |\n",
      "|    explained_variance   | -0.00815     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0067      |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    std                  | 0.594        |\n",
      "|    value_loss           | 4.19e-05     |\n",
      "------------------------------------------\n",
      "Episode: 37, LNN loss: 0.514983057975769, Rewards Loss: 5.687860012054443\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 772         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007975262 |\n",
      "|    clip_fraction        | 0.0274      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.881      |\n",
      "|    explained_variance   | -0.00291    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0217     |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.000179   |\n",
      "|    std                  | 0.577       |\n",
      "|    value_loss           | 4.2e-05     |\n",
      "-----------------------------------------\n",
      "Episode: 38, LNN loss: 0.6046998500823975, Rewards Loss: 5.183527946472168\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 855          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 79872        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014468987 |\n",
      "|    clip_fraction        | 0.0306       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.861       |\n",
      "|    explained_variance   | 0.00162      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.00378     |\n",
      "|    n_updates            | 380          |\n",
      "|    policy_gradient_loss | 0.00077      |\n",
      "|    std                  | 0.57         |\n",
      "|    value_loss           | 4.65e-05     |\n",
      "------------------------------------------\n",
      "Episode: 39, LNN loss: 0.39723414182662964, Rewards Loss: 5.587190628051758\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 761         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001885551 |\n",
      "|    clip_fraction        | 0.0386      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.839      |\n",
      "|    explained_variance   | -0.00219    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00116     |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0006     |\n",
      "|    std                  | 0.551       |\n",
      "|    value_loss           | 2.55e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 40, LNN loss: 0.170514315366745, Rewards Loss: 5.074597358703613\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 663         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003600233 |\n",
      "|    clip_fraction        | 0.0686      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.809      |\n",
      "|    explained_variance   | -0.000371   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0142      |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0015     |\n",
      "|    std                  | 0.537       |\n",
      "|    value_loss           | 4.51e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 41, LNN loss: 0.15790575742721558, Rewards Loss: 4.867812156677246\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 649         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007367186 |\n",
      "|    clip_fraction        | 0.0426      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.787      |\n",
      "|    explained_variance   | -0.00429    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00218     |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | 0.000996    |\n",
      "|    std                  | 0.528       |\n",
      "|    value_loss           | 8.35e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 42, LNN loss: 0.17053407430648804, Rewards Loss: 5.048314094543457\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 722          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 88064        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074700625 |\n",
      "|    clip_fraction        | 0.0323       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.762       |\n",
      "|    explained_variance   | -0.00525     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.000189     |\n",
      "|    n_updates            | 420          |\n",
      "|    policy_gradient_loss | -0.000176    |\n",
      "|    std                  | 0.513        |\n",
      "|    value_loss           | 3.31e-05     |\n",
      "------------------------------------------\n",
      "Episode: 43, LNN loss: 0.1505841314792633, Rewards Loss: 5.082259178161621\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 718         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005717605 |\n",
      "|    clip_fraction        | 0.0684      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.739      |\n",
      "|    explained_variance   | -0.00389    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.0171     |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | 0.000555    |\n",
      "|    std                  | 0.503       |\n",
      "|    value_loss           | 2.95e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 44, LNN loss: 0.19371449947357178, Rewards Loss: 4.80839729309082\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 708         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008446667 |\n",
      "|    clip_fraction        | 0.0594      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.715      |\n",
      "|    explained_variance   | -0.00753    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00914    |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.000452   |\n",
      "|    std                  | 0.488       |\n",
      "|    value_loss           | 3.62e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 45, LNN loss: 0.44058501720428467, Rewards Loss: 5.377162456512451\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 763          |\n",
      "|    iterations           | 1            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 94208        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033245194 |\n",
      "|    clip_fraction        | 0.0343       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.688       |\n",
      "|    explained_variance   | -0.000544    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.000899    |\n",
      "|    n_updates            | 450          |\n",
      "|    policy_gradient_loss | 0.00183      |\n",
      "|    std                  | 0.477        |\n",
      "|    value_loss           | 2.9e-05      |\n",
      "------------------------------------------\n",
      "Episode: 46, LNN loss: 0.44640254974365234, Rewards Loss: 5.328156471252441\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 719         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006491476 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.67       |\n",
      "|    explained_variance   | -0.00637    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | -0.00952    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | 0.00197     |\n",
      "|    std                  | 0.469       |\n",
      "|    value_loss           | 3.27e-05    |\n",
      "-----------------------------------------\n",
      "Episode: 47, LNN loss: 0.4623008966445923, Rewards Loss: 4.821532249450684\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 793        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 2          |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00739364 |\n",
      "|    clip_fraction        | 0.0747     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.643     |\n",
      "|    explained_variance   | -0.0104    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.0182     |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.00107   |\n",
      "|    std                  | 0.453      |\n",
      "|    value_loss           | 3.9e-05    |\n",
      "----------------------------------------\n",
      "Episode: 48, LNN loss: 0.5236446261405945, Rewards Loss: 4.637670516967773\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 655        |\n",
      "|    iterations           | 1          |\n",
      "|    time_elapsed         | 3          |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01794229 |\n",
      "|    clip_fraction        | 0.0625     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.61      |\n",
      "|    explained_variance   | -5.5e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.00767    |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | 0.000831   |\n",
      "|    std                  | 0.438      |\n",
      "|    value_loss           | 2.77e-05   |\n",
      "----------------------------------------\n",
      "Episode: 49, LNN loss: 0.528160810470581, Rewards Loss: 4.656244277954102\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 797         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009621566 |\n",
      "|    clip_fraction        | 0.0711      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | -0.00115    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.0467      |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.000968   |\n",
      "|    std                  | 0.418       |\n",
      "|    value_loss           | 4.24e-05    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 211\u001b[0m\n\u001b[1;32m    208\u001b[0m model\u001b[38;5;241m.\u001b[39mset_logger(new_logger)\n\u001b[1;32m    210\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m trained_model, trained_lnn, trained_reward \u001b[38;5;241m=\u001b[39m train_with_lnn_and_ppo(\n\u001b[1;32m    212\u001b[0m     model,  env, custom_env, device, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Save models\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# torch.save(custom_env.lnn_model.state_dict(), \"lnn_cartpole.pth\")\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# torch.save(custom_env.reward_model.state_dict(), \"reward_cartpole.pth\")\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# trained_model.save(\"ppo_cartpole_with_lnn\")\u001b[39;00m\n\u001b[1;32m    219\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_cartpole_with_lnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class LNNModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(LNNModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, state_dim)  # Predicts next state\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # print(\">>>>\", state.shape, action.shape)\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class RewardNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=256):\n",
    "        super(RewardNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Predicts reward\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.float32),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.tensor(next_states, dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class CustomGymEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gym environment that uses LNN for state transitions, Reward Network for reward calculations,\n",
    "    and incorporates a replay buffer.\n",
    "    \"\"\"\n",
    "    def __init__(self, lnn_model, reward_model, state_dim, action_dim, action_space, observation_space, device, replay_buffer):\n",
    "        super(CustomGymEnvironment, self).__init__()\n",
    "        self.lnn_model = lnn_model\n",
    "        self.reward_model = reward_model\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        # Define observation and action space\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # Initialize state\n",
    "        self.state = np.zeros(state_dim)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # print(\"<<<\", self.state.shape)\n",
    "        if len(self.replay_buffer) > 0:\n",
    "            # Sample a random state from the replay buffer\n",
    "            sample = self.replay_buffer.sample(1)\n",
    "            # print(sample.shape)\n",
    "            if sample:\n",
    "                self.state = sample[0][0]\n",
    "        else:\n",
    "            # Otherwise, initialize randomly\n",
    "            self.state = self.observation_space.sample()\n",
    "            # print(self.state.shape)\n",
    "        # print(\"<<<2\", self.state.shape)\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Convert state and action to tensors\n",
    "        # print(action.shape)\n",
    "        state_tensor = torch.tensor(self.state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        action_tensor = torch.tensor(action, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        # print(\"Hii\",state_tensor.shape, action_tensor.shape)\n",
    "        # Use LNN to predict the next state\n",
    "        next_state = self.lnn_model(state_tensor, action_tensor).squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "        # Use Reward Network to calculate reward\n",
    "        reward = (\n",
    "            self.reward_model(\n",
    "                torch.tensor(next_state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            )\n",
    "            .item()\n",
    "        )\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = np.abs(self.state[0])  <= 0.1 and np.abs(self.state[1]) <= 0.01   # Example condition\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        self.replay_buffer.add(self.state, action, reward, next_state, done)\n",
    "\n",
    "        # Update state\n",
    "        self.state = next_state\n",
    "\n",
    "        return next_state, reward, done, False, {}\n",
    "\n",
    "\n",
    "def train_with_lnn_and_ppo(\n",
    "    model, env, custom_env, device, num_episodes, batch_size\n",
    "):\n",
    "    reward_optimizer = optim.Adam(reward_network.parameters(), lr=2e-6)\n",
    "    lnn_optimizer = optim.Adam(custom_env.lnn_model.parameters(), lr=2e-6)\n",
    "    reward_loss_fn = nn.MSELoss()\n",
    "    lnn_loss_fn = nn.MSELoss()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Collect transitions using PPO policy\n",
    "            action, _ = model.predict(state, deterministic=False)\n",
    "            state1, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # print(state.shape, action.shape)\n",
    "            custom_env.replay_buffer.add(state, action, reward, state1, done)\n",
    "\n",
    "\n",
    "        # Train LNN and Reward Network\n",
    "        if len(custom_env.replay_buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = custom_env.replay_buffer.sample(batch_size)\n",
    "\n",
    "            # Train LNN\n",
    "            lnn_preds = custom_env.lnn_model(states.to(device), actions.to(device))\n",
    "            lnn_loss = lnn_loss_fn(lnn_preds, next_states.to(device))\n",
    "            lnn_optimizer.zero_grad()\n",
    "            lnn_loss.backward()\n",
    "            lnn_optimizer.step()\n",
    "\n",
    "            # Train Reward Network\n",
    "            reward_preds = custom_env.reward_model(next_states.to(device))\n",
    "            reward_loss = reward_loss_fn(reward_preds, rewards.to(device).unsqueeze(-1))\n",
    "            reward_optimizer.zero_grad()\n",
    "            reward_loss.backward()\n",
    "            reward_optimizer.step()\n",
    "\n",
    "            print(f\"Episode: {episode}, LNN loss: {lnn_loss}, Rewards Loss: {reward_loss}\")\n",
    "        # Train PPO policy\n",
    "        model.learn(total_timesteps=2048, reset_num_timesteps=False)\n",
    "\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "tmp_path = \"./\"\n",
    "# set up logger\n",
    "new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "\n",
    "\n",
    "# Setup\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "# print(\"Action Dim\", action_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lnn_model = LNNModel(state_dim, action_dim).to(device)\n",
    "reward_network = RewardNetwork(state_dim).to(device)\n",
    "replay_buffer = ReplayBuffer(max_size=10000)\n",
    "\n",
    "action_space = env.action_space\n",
    "\n",
    "custom_env = CustomGymEnvironment(\n",
    "    lnn_model=lnn_model,\n",
    "    reward_model=reward_network,\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    action_space=action_space,\n",
    "    observation_space=env.observation_space,\n",
    "    device=device,\n",
    "    replay_buffer=replay_buffer,\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", custom_env, verbose=1)\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Train\n",
    "trained_model, trained_lnn, trained_reward = train_with_lnn_and_ppo(\n",
    "    model,  env, custom_env, device, num_episodes=50, batch_size=4096\n",
    ")\n",
    "\n",
    "# Save models\n",
    "# torch.save(custom_env.lnn_model.state_dict(), \"lnn_cartpole.pth\")\n",
    "# torch.save(custom_env.reward_model.state_dict(), \"reward_cartpole.pth\")\n",
    "# trained_model.save(\"ppo_cartpole_with_lnn\")\n",
    "model.save(\"ppo_cartpole_with_lnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_cartpole_with_lnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -1080.87 +/- 308.68\n",
      "Done  199\n",
      "Done  399\n",
      "Done  599\n",
      "Done  799\n",
      "Done  999\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\", render_mode = 'human')\n",
    "# 5. Load the trained model (optional)\n",
    "model = PPO.load(\"ppo_cartpole_with_lnn\", env=env)\n",
    "\n",
    "# 6. Evaluate the trained policy\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=3)\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# 7. Run the trained agent\n",
    "obs, _ = env.reset()\n",
    "for i in range(1000):  # Run for a fixed number of timesteps\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "        print(\"Done \", i)\n",
    "    # if not i%20: print(i)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
