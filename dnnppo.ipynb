{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mayaank\\AppData\\Local\\Temp\\ipykernel_17648\\3534260065.py:104: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(self.state, dtype=torch.float32, device=self.device).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 299  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 316  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 4096 |\n",
      "-----------------------------\n",
      "Episode: 2, LNN loss: 1.0089943408966064, Rewards Loss: 5.690506935119629\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 320  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 6144 |\n",
      "-----------------------------\n",
      "Episode: 3, LNN loss: 0.8061116933822632, Rewards Loss: 4.7942352294921875\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 317  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 6    |\n",
      "|    total_timesteps | 8192 |\n",
      "-----------------------------\n",
      "Episode: 4, LNN loss: 0.7477186918258667, Rewards Loss: 4.771503925323486\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 320   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "Episode: 5, LNN loss: 0.7341070175170898, Rewards Loss: 4.815556526184082\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 321   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 12288 |\n",
      "------------------------------\n",
      "Episode: 6, LNN loss: 0.815514087677002, Rewards Loss: 5.071807384490967\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 317   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 14336 |\n",
      "------------------------------\n",
      "Episode: 7, LNN loss: 0.8143260478973389, Rewards Loss: 4.81221342086792\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 309   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 16384 |\n",
      "------------------------------\n",
      "Episode: 8, LNN loss: 0.7379118800163269, Rewards Loss: 5.651688575744629\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 317   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 18432 |\n",
      "------------------------------\n",
      "Episode: 9, LNN loss: 0.7106520533561707, Rewards Loss: 6.501916885375977\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 314   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n",
      "Episode: 10, LNN loss: 0.6169476509094238, Rewards Loss: 7.110188007354736\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 307   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 22528 |\n",
      "------------------------------\n",
      "Episode: 11, LNN loss: 0.6116361618041992, Rewards Loss: 6.743021488189697\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 311   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 24576 |\n",
      "------------------------------\n",
      "Episode: 12, LNN loss: 0.3979424834251404, Rewards Loss: 7.141209602355957\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 315   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 26624 |\n",
      "------------------------------\n",
      "Episode: 13, LNN loss: 0.43792223930358887, Rewards Loss: 7.851925849914551\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 318   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 28672 |\n",
      "------------------------------\n",
      "Episode: 14, LNN loss: 0.8532590270042419, Rewards Loss: 8.062074661254883\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 319   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 30720 |\n",
      "------------------------------\n",
      "Episode: 15, LNN loss: 0.7150863409042358, Rewards Loss: 8.235589981079102\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 310   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 32768 |\n",
      "------------------------------\n",
      "Episode: 16, LNN loss: 0.4578123986721039, Rewards Loss: 7.4565229415893555\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 300   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 34816 |\n",
      "------------------------------\n",
      "Episode: 17, LNN loss: 0.49728676676750183, Rewards Loss: 6.945516586303711\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 276   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 36864 |\n",
      "------------------------------\n",
      "Episode: 18, LNN loss: 0.7706400752067566, Rewards Loss: 6.210944175720215\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 309   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 38912 |\n",
      "------------------------------\n",
      "Episode: 19, LNN loss: 0.5478857755661011, Rewards Loss: 7.289089679718018\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 324   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 40960 |\n",
      "------------------------------\n",
      "Episode: 20, LNN loss: 0.572568953037262, Rewards Loss: 7.477548599243164\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 290   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 43008 |\n",
      "------------------------------\n",
      "Episode: 21, LNN loss: 0.4721246659755707, Rewards Loss: 6.526597023010254\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 302   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 45056 |\n",
      "------------------------------\n",
      "Episode: 22, LNN loss: 0.7078384160995483, Rewards Loss: 7.411185264587402\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 354   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 5     |\n",
      "|    total_timesteps | 47104 |\n",
      "------------------------------\n",
      "Episode: 23, LNN loss: 0.3658410906791687, Rewards Loss: 6.747336387634277\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 338   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 49152 |\n",
      "------------------------------\n",
      "Episode: 24, LNN loss: 0.6429819464683533, Rewards Loss: 6.606131553649902\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 346   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 5     |\n",
      "|    total_timesteps | 51200 |\n",
      "------------------------------\n",
      "Episode: 25, LNN loss: 0.7100642919540405, Rewards Loss: 6.166974067687988\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 329   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 53248 |\n",
      "------------------------------\n",
      "Episode: 26, LNN loss: 0.6967501640319824, Rewards Loss: 6.614448547363281\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 304   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 55296 |\n",
      "------------------------------\n",
      "Episode: 27, LNN loss: 0.42800769209861755, Rewards Loss: 5.897181987762451\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 314   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 57344 |\n",
      "------------------------------\n",
      "Episode: 28, LNN loss: 0.4902549684047699, Rewards Loss: 5.688225746154785\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 303   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 59392 |\n",
      "------------------------------\n",
      "Episode: 29, LNN loss: 0.48169273138046265, Rewards Loss: 5.617691993713379\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 303   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 61440 |\n",
      "------------------------------\n",
      "Episode: 30, LNN loss: 0.4659571051597595, Rewards Loss: 5.198395729064941\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 302   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 63488 |\n",
      "------------------------------\n",
      "Episode: 31, LNN loss: 0.6801857352256775, Rewards Loss: 5.95936393737793\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 345   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 5     |\n",
      "|    total_timesteps | 65536 |\n",
      "------------------------------\n",
      "Episode: 32, LNN loss: 0.7104741930961609, Rewards Loss: 5.652482032775879\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 322   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 67584 |\n",
      "------------------------------\n",
      "Episode: 33, LNN loss: 0.7253268957138062, Rewards Loss: 6.439425468444824\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 283   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 69632 |\n",
      "------------------------------\n",
      "Episode: 34, LNN loss: 0.43504783511161804, Rewards Loss: 5.938879489898682\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 306   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 71680 |\n",
      "------------------------------\n",
      "Episode: 35, LNN loss: 0.5302214026451111, Rewards Loss: 5.696855545043945\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 315   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 73728 |\n",
      "------------------------------\n",
      "Episode: 36, LNN loss: 0.215036541223526, Rewards Loss: 5.331730842590332\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 313   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 75776 |\n",
      "------------------------------\n",
      "Episode: 37, LNN loss: 0.17544573545455933, Rewards Loss: 5.950290679931641\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 311   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 77824 |\n",
      "------------------------------\n",
      "Episode: 38, LNN loss: 0.21303874254226685, Rewards Loss: 5.464470863342285\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 293   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 79872 |\n",
      "------------------------------\n",
      "Episode: 39, LNN loss: 0.4465540647506714, Rewards Loss: 5.755182266235352\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 291   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 81920 |\n",
      "------------------------------\n",
      "Episode: 40, LNN loss: 0.33014440536499023, Rewards Loss: 6.693645477294922\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 294   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 83968 |\n",
      "------------------------------\n",
      "Episode: 41, LNN loss: 0.36014336347579956, Rewards Loss: 6.603032112121582\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 313   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 86016 |\n",
      "------------------------------\n",
      "Episode: 42, LNN loss: 0.4159119129180908, Rewards Loss: 5.852084636688232\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 305   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 88064 |\n",
      "------------------------------\n",
      "Episode: 43, LNN loss: 0.3933637738227844, Rewards Loss: 6.33001708984375\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 277   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 90112 |\n",
      "------------------------------\n",
      "Episode: 44, LNN loss: 0.4742961823940277, Rewards Loss: 6.625965118408203\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 300   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 92160 |\n",
      "------------------------------\n",
      "Episode: 45, LNN loss: 0.5813513398170471, Rewards Loss: 5.561793327331543\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 303   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 6     |\n",
      "|    total_timesteps | 94208 |\n",
      "------------------------------\n",
      "Episode: 46, LNN loss: 0.8810076713562012, Rewards Loss: 5.974908351898193\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 278   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 96256 |\n",
      "------------------------------\n",
      "Episode: 47, LNN loss: 0.84275883436203, Rewards Loss: 6.159976005554199\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 281   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 7     |\n",
      "|    total_timesteps | 98304 |\n",
      "------------------------------\n",
      "Episode: 48, LNN loss: 0.771719217300415, Rewards Loss: 5.434199810028076\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class LNNModel(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super(LNNModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, state_dim)  # Predicts next state\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # print(\">>>>\", state.shape, action.shape)\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class RewardNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=256):\n",
    "        super(RewardNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Predicts reward\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.float32),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.tensor(next_states, dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class CustomGymEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gym environment that uses LNN for state transitions, Reward Network for reward calculations,\n",
    "    and incorporates a replay buffer.\n",
    "    \"\"\"\n",
    "    def __init__(self, lnn_model, reward_model, state_dim, action_dim, action_space, observation_space, device, replay_buffer):\n",
    "        super(CustomGymEnvironment, self).__init__()\n",
    "        self.lnn_model = lnn_model\n",
    "        self.reward_model = reward_model\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        # Define observation and action space\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # Initialize state\n",
    "        self.state = np.zeros(state_dim)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # print(\"<<<\", self.state.shape)\n",
    "        if len(self.replay_buffer) > 0:\n",
    "            # Sample a random state from the replay buffer\n",
    "            sample = self.replay_buffer.sample(1)\n",
    "            # print(sample.shape)\n",
    "            if sample:\n",
    "                self.state = sample[0][0]\n",
    "        else:\n",
    "            # Otherwise, initialize randomly\n",
    "            self.state = self.observation_space.sample()\n",
    "            # print(self.state.shape)\n",
    "        # print(\"<<<2\", self.state.shape)\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Convert state and action to tensors\n",
    "        # print(action.shape)\n",
    "        state_tensor = torch.tensor(self.state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        action_tensor = torch.tensor(action, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        # print(\"Hii\",state_tensor.shape, action_tensor.shape)\n",
    "        # Use LNN to predict the next state\n",
    "        next_state = self.lnn_model(state_tensor, action_tensor).squeeze(0).cpu().detach().numpy()\n",
    "\n",
    "        # Use Reward Network to calculate reward\n",
    "        reward = (\n",
    "            self.reward_model(\n",
    "                torch.tensor(next_state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            )\n",
    "            .item()\n",
    "        )\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = np.abs(self.state[0])  <= 0.1 and np.abs(self.state[1]) <= 0.01   # Example condition\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        self.replay_buffer.add(self.state, action, reward, next_state, done)\n",
    "\n",
    "        # Update state\n",
    "        self.state = next_state\n",
    "\n",
    "        return next_state, reward, done, False, {}\n",
    "\n",
    "\n",
    "def train_with_lnn_and_ppo(\n",
    "    model, env, custom_env, device, num_episodes, batch_size\n",
    "):\n",
    "    reward_optimizer = optim.Adam(reward_network.parameters(), lr=2e-6)\n",
    "    lnn_optimizer = optim.Adam(custom_env.lnn_model.parameters(), lr=2e-6)\n",
    "    reward_loss_fn = nn.MSELoss()\n",
    "    lnn_loss_fn = nn.MSELoss()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Collect transitions using PPO policy\n",
    "            action, _ = model.predict(state, deterministic=False)\n",
    "            state1, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # print(state.shape, action.shape)\n",
    "            custom_env.replay_buffer.add(state, action, reward, state1, done)\n",
    "\n",
    "\n",
    "        # Train LNN and Reward Network\n",
    "        if len(custom_env.replay_buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = custom_env.replay_buffer.sample(batch_size)\n",
    "\n",
    "            # Train LNN\n",
    "            lnn_preds = custom_env.lnn_model(states.to(device), actions.to(device))\n",
    "            lnn_loss = lnn_loss_fn(lnn_preds, next_states.to(device))\n",
    "            lnn_optimizer.zero_grad()\n",
    "            lnn_loss.backward()\n",
    "            lnn_optimizer.step()\n",
    "\n",
    "            # Train Reward Network\n",
    "            reward_preds = custom_env.reward_model(next_states.to(device))\n",
    "            reward_loss = reward_loss_fn(reward_preds, rewards.to(device).unsqueeze(-1))\n",
    "            reward_optimizer.zero_grad()\n",
    "            reward_loss.backward()\n",
    "            reward_optimizer.step()\n",
    "\n",
    "            print(f\"Episode: {episode}, LNN loss: {lnn_loss}, Rewards Loss: {reward_loss}\")\n",
    "        # Train PPO policy\n",
    "        model.learn(total_timesteps=2048, reset_num_timesteps=False)\n",
    "\n",
    "\n",
    "\n",
    "# Setup\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "# print(\"Action Dim\", action_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "lnn_model = LNNModel(state_dim, action_dim).to(device)\n",
    "reward_network = RewardNetwork(state_dim).to(device)\n",
    "replay_buffer = ReplayBuffer(max_size=10000)\n",
    "\n",
    "action_space = env.action_space\n",
    "\n",
    "custom_env = CustomGymEnvironment(\n",
    "    lnn_model=lnn_model,\n",
    "    reward_model=reward_network,\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    action_space=action_space,\n",
    "    observation_space=env.observation_space,\n",
    "    device=device,\n",
    "    replay_buffer=replay_buffer,\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", custom_env, verbose=1)\n",
    "\n",
    "# Train\n",
    "trained_model, trained_lnn, trained_reward = train_with_lnn_and_ppo(\n",
    "    model,  env, custom_env, device, num_episodes=100, batch_size=4096\n",
    ")\n",
    "\n",
    "# Save models\n",
    "# torch.save(custom_env.lnn_model.state_dict(), \"lnn_cartpole.pth\")\n",
    "# torch.save(custom_env.reward_model.state_dict(), \"reward_cartpole.pth\")\n",
    "# trained_model.save(\"ppo_cartpole_with_lnn\")\n",
    "model.save(\"ppo_cartpole_with_lnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done  199\n",
      "Done  399\n",
      "Done  599\n",
      "Done  799\n",
      "Done  999\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pendulum-v1\", render_mode = 'human')\n",
    "env.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(state_dim,), dtype=np.float32\n",
    "        )\n",
    "# 5. Load the trained model (optional)\n",
    "model = PPO.load(\"ppo_cartpole_with_lnn\", env=env)\n",
    "\n",
    "# 6. Evaluate the trained policy\n",
    "\n",
    "# 7. Run the trained agent\n",
    "obs, _ = env.reset()\n",
    "for i in range(1000):  # Run for a fixed number of timesteps\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "        print(\"Done \", i)\n",
    "    # if not i%20: print(i)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
