{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from garage import wrap_experiment\n",
    "from garage.envs import GymEnv\n",
    "from garage.experiment.deterministic import set_seed\n",
    "from garage.sampler import LocalSampler\n",
    "from garage.tf.models import GaussianMLPModel\n",
    "from garage.tf.policies import CategoricalMLPPolicy\n",
    "from garage.tf.algos import ModelBasedPolicyOptimization\n",
    "from garage.tf.optimizers import FirstOrderOptimizer\n",
    "from garage.replay_buffer import PathBuffer\n",
    "from garage.sampler.default_worker import DefaultWorker\n",
    "from garage.experiment.task_sampler import EnvPoolSampler\n",
    "\n",
    "\n",
    "@wrap_experiment\n",
    "def mb_rl_cartpole(ctxt=None, seed=42):\n",
    "    \"\"\"\n",
    "    Model-Based RL for CartPole using Garage.\n",
    "    Args:\n",
    "        ctxt: Experiment context for saving results.\n",
    "        seed: Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # Set the random seed for reproducibility\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Load the CartPole environment\n",
    "    env = GymEnv(\"CartPole-v1\")\n",
    "    env.spec.max_episode_length = 200\n",
    "    \n",
    "    # Define the dynamics model\n",
    "    dynamics_model = GaussianMLPModel(\n",
    "        input_dim=env.spec.observation_space.flat_dim + env.spec.action_space.flat_dim,\n",
    "        output_dim=env.spec.observation_space.flat_dim,\n",
    "        hidden_sizes=(64, 64),\n",
    "        hidden_nonlinearity=tf.nn.relu,\n",
    "        output_nonlinearity=None,\n",
    "        name=\"DynamicsModel\"\n",
    "    )\n",
    "    \n",
    "    # Define the policy\n",
    "    policy = CategoricalMLPPolicy(\n",
    "        env_spec=env.spec,\n",
    "        hidden_sizes=(64, 64),\n",
    "        hidden_nonlinearity=tf.nn.relu\n",
    "    )\n",
    "    \n",
    "    # Define the optimizer for the dynamics model\n",
    "    dynamics_optimizer = FirstOrderOptimizer(\n",
    "        learning_rate=1e-3,\n",
    "        max_epochs=50\n",
    "    )\n",
    "    \n",
    "    # Replay buffer to store experiences\n",
    "    replay_buffer = PathBuffer(capacity_in_transitions=100000)\n",
    "    \n",
    "    # Sampler for collecting trajectories\n",
    "    sampler = LocalSampler(agents=policy, envs=env, max_episode_length=env.spec.max_episode_length)\n",
    "    \n",
    "    # Define the MBRL algorithm\n",
    "    algo = ModelBasedPolicyOptimization(\n",
    "        env_spec=env.spec,\n",
    "        dynamics_model=dynamics_model,\n",
    "        policy=policy,\n",
    "        dynamics_optimizer=dynamics_optimizer,\n",
    "        buffer=replay_buffer,\n",
    "        sampler=sampler,\n",
    "        imagination_horizon=5,  # Number of steps to simulate in the learned model\n",
    "        discount=0.99,\n",
    "        entropy_regularization=1e-3\n",
    "    )\n",
    "    \n",
    "    # Train the algorithm\n",
    "    algo.train(n_epochs=100, batch_size=4000)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mb_rl_cartpole()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
