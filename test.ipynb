{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "import pygame\n",
    "import time\n",
    "import sys\n",
    "from win32gui import SetWindowPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_DEFAULT_VALUE_AT_MARGIN = 0.1\n",
    "\n",
    "\n",
    "def _sigmoids(x, value_at_1, sigmoid):\n",
    "    if sigmoid in ('cosine', 'linear', 'quadratic'):\n",
    "        if not 0 <= value_at_1 < 1:\n",
    "            raise ValueError('`value_at_1` must be nonnegative and smaller than 1, '\n",
    "                                             'got {}.'.format(value_at_1))\n",
    "    else:\n",
    "        if not 0 < value_at_1 < 1:\n",
    "            raise ValueError('`value_at_1` must be strictly between 0 and 1, '\n",
    "                                             'got {}.'.format(value_at_1))\n",
    "\n",
    "    if sigmoid == 'gaussian':\n",
    "        scale = np.sqrt(-2 * np.log(value_at_1))\n",
    "        return np.exp(-0.5 * (x*scale)**2)\n",
    "\n",
    "    elif sigmoid == 'hyperbolic':\n",
    "        scale = np.arccosh(1/value_at_1)\n",
    "        return 1 / np.cosh(x*scale)\n",
    "\n",
    "    elif sigmoid == 'long_tail':\n",
    "        scale = np.sqrt(1/value_at_1 - 1)\n",
    "        return 1 / ((x*scale)**2 + 1)\n",
    "\n",
    "    elif sigmoid == 'reciprocal':\n",
    "        scale = 1/value_at_1 - 1\n",
    "        return 1 / (abs(x)*scale + 1)\n",
    "\n",
    "    elif sigmoid == 'cosine':\n",
    "        scale = np.arccos(2*value_at_1 - 1) / np.pi\n",
    "        scaled_x = x*scale\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                    action='ignore', message='invalid value encountered in cos')\n",
    "            cos_pi_scaled_x = np.cos(np.pi*scaled_x)\n",
    "        return np.where(abs(scaled_x) < 1, (1 + cos_pi_scaled_x)/2, 0.0)\n",
    "\n",
    "    elif sigmoid == 'linear':\n",
    "        scale = 1-value_at_1\n",
    "        scaled_x = x*scale\n",
    "        return np.where(abs(scaled_x) < 1, 1 - scaled_x, 0.0)\n",
    "\n",
    "    elif sigmoid == 'quadratic':\n",
    "        scale = np.sqrt(1-value_at_1)\n",
    "        scaled_x = x*scale\n",
    "        return np.where(abs(scaled_x) < 1, 1 - scaled_x**2, 0.0)\n",
    "\n",
    "    elif sigmoid == 'tanh_squared':\n",
    "        scale = np.arctanh(np.sqrt(1-value_at_1))\n",
    "        return 1 - np.tanh(x*scale)**2\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Unknown sigmoid type {!r}.'.format(sigmoid))\n",
    "\n",
    "\n",
    "def tolerance(x, bounds=(0.0, 0.0), margin=0.0, sigmoid='gaussian',\n",
    "                            value_at_margin=_DEFAULT_VALUE_AT_MARGIN):\n",
    "    lower, upper = bounds\n",
    "    if lower > upper:\n",
    "        raise ValueError('Lower bound must be <= upper bound.')\n",
    "    if margin < 0:\n",
    "        raise ValueError('`margin` must be non-negative.')\n",
    "\n",
    "    in_bounds = np.logical_and(lower <= x, x <= upper)\n",
    "    if margin == 0:\n",
    "        value = np.where(in_bounds, 1.0, 0.0)\n",
    "    else:\n",
    "        d = np.where(x < lower, lower - x, x - upper) / margin\n",
    "        value = np.where(in_bounds, 1.0, _sigmoids(d, value_at_margin, sigmoid))\n",
    "\n",
    "    return float(value) if np.isscalar(x) else value\n",
    "\n",
    "def wrap(x):\n",
    "    return ((x + np.pi) % (2 * np.pi)) - np.pi\n",
    "\n",
    "def rect_points(center, length, width, ang, scaling, offset):\n",
    "    points = []\n",
    "    diag = np.sqrt(length**2+width**2)/2\n",
    "    ang1 = 2*np.arctan2(width,length)\n",
    "    ang2 = 2*np.arctan2(length,width)\n",
    "\n",
    "    points.append((center[0]+np.sin(ang+ang1/2)*diag, center[1]+np.cos(ang+ang1/2)*diag))   \n",
    "    \n",
    "    points.append((center[0]+np.sin(ang+ang1/2+ang2)*diag, center[1]+np.cos(ang+ang1/2+ang2)*diag))\n",
    "    \n",
    "    points.append((center[0]+np.sin(ang+ang1*1.5+ang2)*diag, center[1]+np.cos(ang+ang1*1.5+ang2)*diag))\n",
    "    \n",
    "    points.append((center[0]+np.sin(ang+ang1*1.5+2*ang2)*diag, center[1]+np.cos(ang+ang1*1.5+2*ang2)*diag))\n",
    "    \n",
    "    return [pygame_transform(point, scaling, offset) for point in points]\n",
    "\n",
    "def pygame_transform(point, scaling, offset):\n",
    "    # Pygame's y axis points downwards. Hence invert y coordinate alone before offset.\n",
    "    return (offset[0]+scaling*point[0],offset[1]-scaling*point[1]) \n",
    "\n",
    "def create_background(length, width):\n",
    "    background = pygame.Surface((length, width))\n",
    "    pygame.draw.rect(background, (0,0,0), pygame.Rect(0, 0, length, width))\n",
    "    return background\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPole:\n",
    "    def __init__(self):\n",
    "\n",
    "        m1 = 1\n",
    "        l1, r1, I1 = 0, 0, 0 # dummy\n",
    "        \n",
    "        m2 = 0.1\n",
    "        # m2 = 1e-10\n",
    "        l2 = 1\n",
    "        r2 = l2/2\n",
    "        I2 = m2 * l2**2 / 12\n",
    "        \n",
    "        g = 9.8\n",
    "\n",
    "        m = [m1,m2]\n",
    "        l = [l1,l2]\n",
    "        r = [r1,r2]\n",
    "        I = [I1,I2]\n",
    "\n",
    "        self.name = \"Cart Pole\"\n",
    "        self.n = 2\n",
    "        self.obs_size = 5\n",
    "        self.action_size = 1\n",
    "        self.inertials = m+l+r+I+[g]\n",
    "        self.a_scale = np.array([10.0])\n",
    "\n",
    "        # For rendering\n",
    "        self.display = False\n",
    "        self.screen_width = 500\n",
    "        self.screen_height = 500\n",
    "        self.offset = [250, 250]\n",
    "        self.scaling = 75\n",
    "        self.x_limit = 2.0\n",
    "\n",
    "        \n",
    "        self.link_length = 1.0\n",
    "        self.link_width = 0.2\n",
    "        self.link_color = (72,209,204) # medium turquoise\n",
    "\n",
    "        self.joint_radius = self.link_width/1.8\n",
    "        self.joint_color = (205,55,0) # orange red\n",
    "\n",
    "        self.cart_length = 5*self.link_width\n",
    "        self.cart_width = 2*self.link_width\n",
    "        self.cart_color = (200,255,0) # yellow\n",
    "\n",
    "        self.rail_length = 2*self.x_limit\n",
    "        self.rail_width = self.link_width/2.5\n",
    "        self.rail_color = (150,150,150) # gray\n",
    "\n",
    "\n",
    "\n",
    "        self.dt = 0.01 # time per simulation step (in seconds)\n",
    "        self.t = 0 # elapsed simulation steps\n",
    "        self.t_max = 10000000 # max simulation steps\n",
    "        self.state = np.zeros(self.n)\n",
    "        self.ang_vel_limit = 20.0\n",
    "        with open(\"robot.p\", \"rb\") as inf:\n",
    "            funcs = pickle.load(inf)                \n",
    "        self.kinematics = funcs['kinematics']\n",
    "        self.dynamics = funcs['dynamics']\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def wrap_state(self):\n",
    "        self.state[1] = wrap(self.state[1])\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.state = np.array([0.01*np.random.randn(),\n",
    "                               np.pi + 0.01*np.random.randn(),\n",
    "                               0,0])\n",
    "\n",
    "    def get_A(self, a):\n",
    "        a_1, = np.clip(a, -1.0, 1.0)*self.a_scale\n",
    "        a_2 = 0.0\n",
    "        return np.array([a_1,a_2])\n",
    "\n",
    "    def get_obs(self):\n",
    "        return np.array([self.state[0],\n",
    "                        np.cos(self.state[1]),np.sin(self.state[1]),\n",
    "                        self.state[2],\n",
    "                        self.state[3]\n",
    "                        ])\n",
    "\n",
    "    def get_reward(self):\n",
    "        upright = (np.array([np.cos(self.state[1])])+1)/2\n",
    "\n",
    "        if np.abs(self.state[0]) <= self.x_limit:\n",
    "            centered = tolerance(self.state[0], margin=self.x_limit)\n",
    "            centered = (1 + centered) / 2\n",
    "        else:\n",
    "            centered = 0.1\n",
    "\n",
    "        qdot = self.state[self.n:]\n",
    "        ang_vel = qdot\n",
    "        small_velocity = tolerance(ang_vel[1:], margin=self.ang_vel_limit/2).min()\n",
    "        small_velocity = (1 + small_velocity) / 2\n",
    "\n",
    "        reward = upright.mean() * small_velocity * centered\n",
    "\n",
    "        return reward  \n",
    "\n",
    "    def reset(self):\n",
    "        self.reset_state()\n",
    "        self.wrap_state()\n",
    "        self.geo = self.kinematics(self.inertials+self.state.tolist())\n",
    "        self.t = 0\n",
    "\n",
    "        return self.get_obs(), 0.0, False\n",
    "\n",
    "    def step(self, a):\n",
    "        self.state = self.rk4(self.state, self.get_A(a))\n",
    "        self.wrap_state()\n",
    "        self.geo = self.kinematics(self.inertials+self.state.tolist())\n",
    "        \n",
    "        self.t += 1\n",
    "        if self.t >= self.t_max: # infinite horizon formulation, no terminal state, similar to dm_control\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        if np.abs(self.state[0]) >= self.x_limit: done = True\n",
    "\n",
    "        return self.get_obs(), self.get_reward(), done\n",
    "\n",
    "    def F(self, s, a):\n",
    "        M, C, G = self.dynamics(self.inertials+s.tolist())\n",
    "        qdot = s[self.n:]\n",
    "        qddot = np.linalg.inv(M+1e-6*np.eye(self.n)) @ (a - C @ qdot - G.flatten()) \n",
    "        \n",
    "        return np.concatenate((qdot,qddot))\n",
    "\n",
    "    def rk4(self, s, a):\n",
    "        s0 = deepcopy(s)\n",
    "        k = []\n",
    "        for l in range(4):\n",
    "            if l > 0:\n",
    "                if l == 1 or l == 2:\n",
    "                    dt = self.dt/2\n",
    "                elif l == 3:\n",
    "                    dt = self.dt\n",
    "                s = s0 + dt * k[l-1]\n",
    "            k.append(self.F(s, a))\n",
    "        s = s0 + (self.dt/6.0) * (k[0] + 2 * k[1] + 2 * k[2] + k[3])\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def render(self):\n",
    "        if self.display:\n",
    "            self.screen.blit(self.background, (0, 0))\n",
    "            self.draw()\n",
    "            time.sleep(0.006)\n",
    "            pygame.display.flip()\n",
    "        else:\n",
    "            self.display = True\n",
    "            pygame.init()\n",
    "            self.screen = pygame.display.set_mode((self.screen_width, self.screen_height))\n",
    "            SetWindowPos(pygame.display.get_wm_info()['window'], -1, 200, 200, 0, 0, 1)\n",
    "            pygame.display.set_caption(self.name)\n",
    "            self.background = create_background(self.screen_width, self.screen_height)\n",
    "\n",
    "\n",
    "    def draw(self):        \n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "        centers, joints, angles = self.geo\n",
    "\n",
    "        #horizontal rail\n",
    "        pygame.draw.polygon(self.screen, self.rail_color, rect_points([0,0], self.rail_length, self.rail_width, np.pi/2, self.scaling, self.offset)) \n",
    "\n",
    "        plot_x = ((centers[0,0] + self.x_limit) % (2 * self.x_limit)) - self.x_limit\n",
    "        link1_points = rect_points([plot_x,0], self.cart_length, self.cart_width, np.pi/2, self.scaling, self.offset) \n",
    "        pygame.draw.polygon(self.screen, self.cart_color, link1_points)\n",
    "        \n",
    "        offset = np.array([plot_x-centers[0,0],0])\n",
    "        for j in range(1,self.n):\n",
    "            link_points = rect_points(centers[j]+offset, self.link_length, self.link_width, angles[j,0],self.scaling,self.offset)\n",
    "            pygame.draw.polygon(self.screen, self.link_color, link_points)\n",
    "        \n",
    "            joint_point = pygame_transform(joints[j]+offset,self.scaling,self.offset)\n",
    "            pygame.draw.circle(self.screen, self.joint_color, joint_point, self.scaling*self.joint_radius)\n",
    "        pygame.display.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9999710075929191\n",
      "0.99997016722535\n",
      "0.9999686499946226\n",
      "0.9999664757631976\n",
      "0.999963706114497\n",
      "0.9999603773549706\n",
      "0.999956451414823\n",
      "0.9999518299099678\n",
      "0.9999464193351973\n",
      "0.9999401890148919\n",
      "0.9999331697765069\n",
      "0.9999253923092746\n",
      "0.9999168184845828\n",
      "0.9999073286554322\n",
      "0.999896778446854\n",
      "0.9998850747430672\n",
      "0.9998722021832472\n",
      "0.9998581732440035\n",
      "0.9998429429326232\n",
      "0.9998263652524819\n",
      "0.999808234003744\n",
      "0.9997883732750561\n",
      "0.9997666959132209\n",
      "0.999743173495856\n",
      "0.9997177388882377\n",
      "0.999690207583729\n",
      "0.9996602926726034\n",
      "0.9996277038774397\n",
      "0.9995922419454584\n",
      "0.9995537989965914\n",
      "0.9995122562804135\n",
      "0.9994673668617733\n",
      "0.9994187324079477\n",
      "0.9993659009211315\n",
      "0.9993084993943834\n",
      "0.999246277322881\n",
      "0.9991790112193497\n",
      "0.999106347438129\n",
      "0.9990277262670012\n",
      "0.9989424632207716\n",
      "0.9988499186504378\n",
      "0.9987495987230106\n",
      "0.9986410836805315\n",
      "0.9985238340415729\n",
      "0.9983970464677832\n",
      "0.9982596971135625\n",
      "0.998110740622423\n",
      "0.9979492826055737\n",
      "0.997774554747989\n",
      "0.997585694642416\n",
      "0.9973815190523169\n",
      "0.9971605010655917\n",
      "0.9969209817001267\n",
      "0.9966614246316108\n",
      "0.9963804670905264\n",
      "0.9960766933528766\n",
      "0.9957483155518362\n",
      "0.9953930504808404\n",
      "0.9950083149413564\n",
      "0.9945915658955463\n",
      "0.9941404601350414\n",
      "0.9936526536714214\n",
      "0.9931253894686211\n",
      "0.9925552366083144\n",
      "0.9919382264330526\n",
      "0.991270268367531\n",
      "0.9905474514817705\n",
      "0.989765915618617\n",
      "0.9889213605371748\n",
      "0.9880086126318085\n",
      "0.9870216451632314\n",
      "0.9859540415320341\n",
      "0.9847994656870364\n",
      "0.983551663660725\n",
      "0.9822039303706026\n",
      "0.980748481106976\n",
      "0.9791762903824937\n",
      "0.9774775524487369\n",
      "0.9756423330682104\n",
      "0.9736607688589262\n",
      "0.9715225560225788\n",
      "0.9692161303175164\n",
      "0.966728263499101\n",
      "0.9640444515874371\n",
      "0.9611497380977582\n",
      "0.9580291789040332\n",
      "0.9546674487653173\n",
      "0.9510478792184872\n",
      "0.9471517813596239\n",
      "0.9429586885463378\n",
      "0.938447315819092\n",
      "0.9335963451406412\n",
      "0.9283842732088933\n",
      "0.9227884217447718\n",
      "0.9167840229213269\n",
      "0.9103442731915926\n",
      "0.9034413762710252\n",
      "0.8960476721261593\n",
      "0.8881358554191533\n",
      "0.8796781413713054\n"
     ]
    }
   ],
   "source": [
    "cp = CartPole()\n",
    "cp.reset_state()\n",
    "cp.state[0] = 0\n",
    "cp.state[1] = 0\n",
    "cp.state[3] = 0.1\n",
    "\n",
    "# cp.state[2] = 0\n",
    "print(cp.state[0])\n",
    "for i in range(100):\n",
    "    print(cp.step(0.001*np.cos(i))[1])\n",
    "    cp.render()\n",
    "\n",
    "pygame.quit()\n",
    "# print(cp.step(0)[0])\n",
    "# print(cp.step(0)[0])\n",
    "# print(cp.step(0)[0])\n",
    "# print(cp.step(0)[0])\n",
    "# print(cp.step(0)[0])\n",
    "# print(cp.step(0)[0])\n",
    "# print(cp.step(0)[0])\n",
    "# print(cp.step(-1)[0])\n",
    "# print(cp.step(-1)[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Hyperparameters\n",
    "STATE_DIM = 5  # [cart position, cos(angle), sin(angle), velocity, angular velocity]\n",
    "ACTION_SPACE = [-1.0, 0.0, 1.0]  # Discretized action space\n",
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 5000\n",
    "MEMORY_CAPACITY = 10000\n",
    "TARGET_UPDATE_FREQ = 500\n",
    "MAX_EPISODES = 500\n",
    "MAX_STEPS = 400\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.out = nn.Linear(128, 1)  # Output a single action in [-1, 1]\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        action = torch.tanh(self.out(x))  # Ensure action is in [-1, 1]\n",
    "        return action\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(next_states),\n",
    "            np.array(dones, dtype=np.float32),\n",
    "        )\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def select_action(state, noise_std=0.1):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    action = policy_net(state).item()\n",
    "    action += np.random.normal(0, noise_std)  # Add Gaussian noise for exploration\n",
    "    return np.clip(action, -1, 1)  # Clamp action to [-1, 1]\n",
    "\n",
    "\n",
    "env = CartPole()\n",
    "policy_net = DQNetwork(STATE_DIM)\n",
    "target_net = DQNetwork(STATE_DIM)\n",
    "\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "replay_buffer = ReplayBuffer(MEMORY_CAPACITY)\n",
    "epsilon = EPSILON_START\n",
    "epsilon_decay = (EPSILON_START - EPSILON_END) / EPSILON_DECAY\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: 7.117726845378057\n",
      "Episode 2, Total Reward: 7.073973744688231\n",
      "Episode 3, Total Reward: 7.099217249753773\n",
      "Episode 4, Total Reward: 7.298702605703624\n",
      "Episode 5, Total Reward: 6.988197217073682\n",
      "Episode 6, Total Reward: 7.125907681304353\n",
      "Episode 7, Total Reward: 6.9289337793996735\n",
      "Episode 8, Total Reward: 6.991233270456222\n",
      "Episode 9, Total Reward: 7.290256011958554\n",
      "Episode 10, Total Reward: 7.116665355714294\n",
      "Episode 11, Total Reward: 7.186852529143123\n",
      "Episode 12, Total Reward: 7.049926426751801\n",
      "Episode 13, Total Reward: 7.215255051750727\n",
      "Episode 14, Total Reward: 7.033314948448251\n",
      "Episode 15, Total Reward: 7.088752036651637\n",
      "Episode 16, Total Reward: 7.173305841726743\n",
      "Episode 17, Total Reward: 7.103817348188387\n",
      "Episode 18, Total Reward: 7.1727839109654745\n",
      "Episode 19, Total Reward: 7.189483936064566\n",
      "Episode 20, Total Reward: 7.235369060389344\n",
      "Episode 21, Total Reward: 7.219348625668055\n",
      "Episode 22, Total Reward: 7.063798217764215\n",
      "Episode 23, Total Reward: 7.0846123728015415\n",
      "Episode 24, Total Reward: 7.175067959770486\n",
      "Episode 25, Total Reward: 7.179982210551677\n",
      "Episode 26, Total Reward: 6.983766054490025\n",
      "Episode 27, Total Reward: 7.175989752308902\n",
      "Episode 28, Total Reward: 6.982083952562469\n",
      "Episode 29, Total Reward: 7.152258153777948\n",
      "Episode 30, Total Reward: 7.119325296677639\n",
      "Episode 31, Total Reward: 7.022197600549769\n",
      "Episode 32, Total Reward: 7.171675022129322\n",
      "Episode 33, Total Reward: 7.183917568624722\n",
      "Episode 34, Total Reward: 7.131108380123644\n",
      "Episode 35, Total Reward: 6.9924369039942675\n",
      "Episode 36, Total Reward: 7.118561106168749\n",
      "Episode 37, Total Reward: 7.1462813095065565\n",
      "Episode 38, Total Reward: 6.976437452749861\n",
      "Episode 39, Total Reward: 7.225321805100544\n",
      "Episode 40, Total Reward: 7.045103152935262\n",
      "Episode 41, Total Reward: 7.079476862343279\n",
      "Episode 42, Total Reward: 7.085409654601632\n",
      "Episode 43, Total Reward: 7.151029670125875\n",
      "Episode 44, Total Reward: 7.007360991294516\n",
      "Episode 45, Total Reward: 7.144677609806997\n",
      "Episode 46, Total Reward: 7.116035559470849\n",
      "Episode 47, Total Reward: 7.10335810355929\n",
      "Episode 48, Total Reward: 7.203695940379774\n",
      "Episode 49, Total Reward: 7.037284951033944\n",
      "Episode 50, Total Reward: 7.07153765034868\n",
      "Episode 51, Total Reward: 7.183742750970832\n",
      "Episode 52, Total Reward: 7.240120769155133\n",
      "Episode 53, Total Reward: 7.042357807679872\n",
      "Episode 54, Total Reward: 7.229878572148739\n",
      "Episode 55, Total Reward: 6.897749348561404\n",
      "Episode 56, Total Reward: 7.136029029248506\n",
      "Episode 57, Total Reward: 7.139570450093112\n",
      "Episode 58, Total Reward: 7.2328098752993135\n",
      "Episode 59, Total Reward: 7.10566934691646\n",
      "Episode 60, Total Reward: 7.0591752910357135\n",
      "Episode 61, Total Reward: 7.075528331815847\n",
      "Episode 62, Total Reward: 7.156592277370841\n",
      "Episode 63, Total Reward: 7.136443552578392\n",
      "Episode 64, Total Reward: 7.166209952683579\n",
      "Episode 65, Total Reward: 7.169382070496826\n",
      "Episode 66, Total Reward: 6.997541777741585\n",
      "Episode 67, Total Reward: 7.086629483144961\n",
      "Episode 68, Total Reward: 7.233917738599198\n",
      "Episode 69, Total Reward: 7.229281256895158\n",
      "Episode 70, Total Reward: 7.184260881289358\n",
      "Episode 71, Total Reward: 7.315655444927909\n",
      "Episode 72, Total Reward: 7.144264571693572\n",
      "Episode 73, Total Reward: 7.18522640009506\n",
      "Episode 74, Total Reward: 7.197701355819711\n",
      "Episode 75, Total Reward: 6.989230816494015\n",
      "Episode 76, Total Reward: 7.073336202202701\n",
      "Episode 77, Total Reward: 7.33014508779493\n",
      "Episode 78, Total Reward: 7.16890747276516\n",
      "Episode 79, Total Reward: 7.020480852760989\n",
      "Episode 80, Total Reward: 7.37359368858055\n",
      "Episode 81, Total Reward: 7.1736465705313615\n",
      "Episode 82, Total Reward: 6.898588971246451\n",
      "Episode 83, Total Reward: 7.118759238704128\n",
      "Episode 84, Total Reward: 7.033917421596364\n",
      "Episode 85, Total Reward: 7.156940143198373\n",
      "Episode 86, Total Reward: 7.113765244450127\n",
      "Episode 87, Total Reward: 7.204209887866745\n",
      "Episode 88, Total Reward: 7.2754873408281\n",
      "Episode 89, Total Reward: 7.099341062451352\n",
      "Episode 90, Total Reward: 7.116320296326521\n",
      "Episode 91, Total Reward: 7.251542116702048\n",
      "Episode 92, Total Reward: 7.048461380457406\n",
      "Episode 93, Total Reward: 7.184470943276998\n",
      "Episode 94, Total Reward: 7.224265927985028\n",
      "Episode 95, Total Reward: 7.286922404880007\n",
      "Episode 96, Total Reward: 7.096778845311564\n",
      "Episode 97, Total Reward: 7.032051125047581\n",
      "Episode 98, Total Reward: 7.349720135971644\n",
      "Episode 99, Total Reward: 7.246177780197573\n",
      "Episode 100, Total Reward: 7.002885215539172\n",
      "Episode 101, Total Reward: 6.986789039833403\n",
      "Episode 102, Total Reward: 7.042159935774364\n",
      "Episode 103, Total Reward: 7.229945732928682\n",
      "Episode 104, Total Reward: 7.19731313200305\n",
      "Episode 105, Total Reward: 7.093901206533754\n",
      "Episode 106, Total Reward: 7.036391142080989\n",
      "Episode 107, Total Reward: 7.149646824910245\n",
      "Episode 108, Total Reward: 7.066567386729489\n",
      "Episode 109, Total Reward: 7.095229381685312\n",
      "Episode 110, Total Reward: 7.1778855344110974\n",
      "Episode 111, Total Reward: 7.026893648543299\n",
      "Episode 112, Total Reward: 7.259644772351707\n",
      "Episode 113, Total Reward: 6.985151414557009\n",
      "Episode 114, Total Reward: 7.20026019086266\n",
      "Episode 115, Total Reward: 7.162068604403627\n",
      "Episode 116, Total Reward: 7.1619577931107665\n",
      "Episode 117, Total Reward: 7.192038989782583\n",
      "Episode 118, Total Reward: 7.15292410712806\n",
      "Episode 119, Total Reward: 7.2401839450712355\n",
      "Episode 120, Total Reward: 7.223868974943057\n",
      "Episode 121, Total Reward: 6.970425718401704\n",
      "Episode 122, Total Reward: 7.160111538024801\n",
      "Episode 123, Total Reward: 7.190909401331607\n",
      "Episode 124, Total Reward: 6.865342993705535\n",
      "Episode 125, Total Reward: 7.248486198943171\n",
      "Episode 126, Total Reward: 7.0307762336106085\n",
      "Episode 127, Total Reward: 7.095108509063756\n",
      "Episode 128, Total Reward: 7.115744660406367\n",
      "Episode 129, Total Reward: 7.177312461974289\n",
      "Episode 130, Total Reward: 7.221640798714094\n",
      "Episode 131, Total Reward: 7.040352451894748\n",
      "Episode 132, Total Reward: 7.09853334269438\n",
      "Episode 133, Total Reward: 7.031766142830637\n",
      "Episode 134, Total Reward: 7.080461799199754\n",
      "Episode 135, Total Reward: 7.052820589844706\n",
      "Episode 136, Total Reward: 7.204519835706705\n",
      "Episode 137, Total Reward: 6.9613852499331195\n",
      "Episode 138, Total Reward: 6.997990009410118\n",
      "Episode 139, Total Reward: 7.17302737482438\n",
      "Episode 140, Total Reward: 6.9787358239847315\n",
      "Episode 141, Total Reward: 7.119328206142289\n",
      "Episode 142, Total Reward: 7.112908798026743\n",
      "Episode 143, Total Reward: 7.253353310045447\n",
      "Episode 144, Total Reward: 7.190730343186728\n",
      "Episode 145, Total Reward: 7.235175808644172\n",
      "Episode 146, Total Reward: 7.099442726774447\n",
      "Episode 147, Total Reward: 7.224868444590338\n",
      "Episode 148, Total Reward: 7.255510573027813\n",
      "Episode 149, Total Reward: 6.895297637220249\n",
      "Episode 150, Total Reward: 7.111989857787508\n",
      "Episode 151, Total Reward: 7.084596284999515\n",
      "Episode 152, Total Reward: 7.038225571292189\n",
      "Episode 153, Total Reward: 7.166887362346559\n",
      "Episode 154, Total Reward: 7.193629835584108\n",
      "Episode 155, Total Reward: 7.095792675926133\n",
      "Episode 156, Total Reward: 7.194062045202423\n",
      "Episode 157, Total Reward: 7.26250849442141\n",
      "Episode 158, Total Reward: 7.246351462433298\n",
      "Episode 159, Total Reward: 7.017787276451758\n",
      "Episode 160, Total Reward: 6.956879016940382\n",
      "Episode 161, Total Reward: 7.185173950898147\n",
      "Episode 162, Total Reward: 6.976109994148442\n",
      "Episode 163, Total Reward: 7.160466033387978\n",
      "Episode 164, Total Reward: 7.204565593582736\n",
      "Episode 165, Total Reward: 7.211421729895059\n",
      "Episode 166, Total Reward: 7.169261694038794\n",
      "Episode 167, Total Reward: 7.211755312714867\n",
      "Episode 168, Total Reward: 7.087290010207753\n",
      "Episode 169, Total Reward: 7.298837746386057\n",
      "Episode 170, Total Reward: 7.207720017458508\n",
      "Episode 171, Total Reward: 7.040822438652044\n",
      "Episode 172, Total Reward: 7.049800408813511\n",
      "Episode 173, Total Reward: 7.163630512170092\n",
      "Episode 174, Total Reward: 7.036930694134858\n",
      "Episode 175, Total Reward: 7.091032513023327\n",
      "Episode 176, Total Reward: 7.217392866571007\n",
      "Episode 177, Total Reward: 7.221526464954391\n",
      "Episode 178, Total Reward: 7.216427359318965\n",
      "Episode 179, Total Reward: 7.2258508133052395\n",
      "Episode 180, Total Reward: 7.214828658332216\n",
      "Episode 181, Total Reward: 7.215864476146326\n",
      "Episode 182, Total Reward: 6.960082360723774\n",
      "Episode 183, Total Reward: 7.270908677961261\n",
      "Episode 184, Total Reward: 6.985049252649878\n",
      "Episode 185, Total Reward: 7.233374978875848\n",
      "Episode 186, Total Reward: 7.313445533712338\n",
      "Episode 187, Total Reward: 7.13272649395444\n",
      "Episode 188, Total Reward: 6.998742718925067\n",
      "Episode 189, Total Reward: 7.16140791409136\n",
      "Episode 190, Total Reward: 7.040299756090712\n",
      "Episode 191, Total Reward: 7.3172926687902\n",
      "Episode 192, Total Reward: 6.897094518515097\n",
      "Episode 193, Total Reward: 7.152005839662399\n",
      "Episode 194, Total Reward: 7.050894724344335\n",
      "Episode 195, Total Reward: 7.075061255759362\n",
      "Episode 196, Total Reward: 7.271957653950855\n",
      "Episode 197, Total Reward: 7.139719539992017\n",
      "Episode 198, Total Reward: 7.157314842950819\n",
      "Episode 199, Total Reward: 7.135655152952975\n",
      "Episode 200, Total Reward: 7.209633288406631\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "MAX_STEPS = 400\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "for episode in range(200):\n",
    "    state, reward, done = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(MAX_STEPS):\n",
    "        # Select action\n",
    "        action = select_action(state, epsilon)\n",
    "        \n",
    "        # Perform action\n",
    "        next_state, reward, done = env.step(action)\n",
    "        # print(next_state.shape)\n",
    "        replay_buffer.add((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps_done += 1\n",
    "\n",
    "        # Decrease epsilon\n",
    "        epsilon = max(EPSILON_END, epsilon - epsilon_decay)\n",
    "\n",
    "        # Train the network\n",
    "        if replay_buffer.size() >= BATCH_SIZE:\n",
    "            # print(replay_buffer.sample(BATCH_SIZE))\n",
    "            states, actions, rewards, next_states, dones = replay_buffer.sample(BATCH_SIZE)\n",
    "            \n",
    "            states_tensor = torch.FloatTensor(states)\n",
    "            actions_tensor = torch.LongTensor(actions).unsqueeze(1)\n",
    "            rewards_tensor = torch.FloatTensor(rewards)\n",
    "            next_states_tensor = torch.FloatTensor(next_states)\n",
    "            dones_tensor = torch.FloatTensor(dones)\n",
    "\n",
    "            # Q-values for the current states\n",
    "            predicted_actions = policy_net(states_tensor).squeeze()\n",
    "            with torch.no_grad():\n",
    "                target_actions = target_net(next_states_tensor).squeeze()\n",
    "                target_values = rewards_tensor + GAMMA * (1 - dones_tensor) * target_actions\n",
    "\n",
    "            loss = F.mse_loss(predicted_actions, target_values)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        if steps_done % TARGET_UPDATE_FREQ == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Total Reward: {total_reward}\")\n",
    "\n",
    "print(\"Training Complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 67 steps with total reward: 7.137649269512246\n",
      "Total Reward from the trained model: 7.137649269512246\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "env.display = False\n",
    "# Function to select the best action (exploitation only, no Îµ-greedy)\n",
    "def select_best_action(state, policy_net):\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "    action = policy_net(state).item()\n",
    "    return np.clip(action, -1, 1)  # Clamp action to [-1, 1]\n",
    "\n",
    "# Run the model and render the results\n",
    "def run_trained_model(policy_net, env, max_steps=MAX_STEPS):\n",
    "    state, reward, done = env.reset()\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        env.render()  # Display the environment using pygame\n",
    "        action = select_best_action(state, policy_net)\n",
    "        state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode finished after {t + 1} steps with total reward: {total_reward}\")\n",
    "            break\n",
    "\n",
    "    # env.close()\n",
    "    return total_reward\n",
    "\n",
    "env.reset()\n",
    "# Run the model\n",
    "total_reward = run_trained_model(policy_net, env)\n",
    "print(f\"Total Reward from the trained model: {total_reward}\")\n",
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_END = 0.01"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
