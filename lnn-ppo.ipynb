{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from icecream import ic\n",
    "\n",
    "#! Set the environment variables to override gpu (specically for my device)\n",
    "os.environ['HSA_OVERRIDE_GFX_VERSION'] = '10.3.0'\n",
    "os.environ['ROCBLAS_TENSILE_LIBRARY'] = '/home/autrio/.local/lib/python3.10/site-packages/torch/lib/rocblas/library/TensileLibrary_lazy_gfx1030.dat'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Dim 1\n",
      "torch.Size([64, 0]) 1 1 3\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65686/3778595034.py:302: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 115  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 17   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004061522 |\n",
      "|    clip_fraction        | 0.0366      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.42       |\n",
      "|    explained_variance   | -0.14       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.00409     |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00277    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 0.416       |\n",
      "-----------------------------------------\n",
      "Episode: 2, LNN loss: 0.6015889644622803, Rewards Loss: 7.643094539642334\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 123         |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002107755 |\n",
      "|    clip_fraction        | 0.00142     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.186       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.09        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.000503   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 28.2        |\n",
      "-----------------------------------------\n",
      "Episode: 3, LNN loss: 0.5786061882972717, Rewards Loss: 6.599545001983643\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 426\u001b[0m\n\u001b[1;32m    423\u001b[0m model\u001b[38;5;241m.\u001b[39mset_logger(new_logger)\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 426\u001b[0m trained_model, trained_lnn, trained_reward \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_lnn_and_ppo\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\n\u001b[1;32m    428\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Save models\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# torch.save(custom_env.lnn_model.state_dict(), \"lnn_cartpole.pth\")\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# torch.save(custom_env.reward_model.state_dict(), \"reward_cartpole.pth\")\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# trained_model.save(\"ppo_cartpole_with_lnn\")\u001b[39;00m\n\u001b[1;32m    434\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_cartpole_with_lnn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 378\u001b[0m, in \u001b[0;36mtrain_with_lnn_and_ppo\u001b[0;34m(model, env, custom_env, device, num_episodes, batch_size)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    376\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, LNN loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlnn_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Rewards Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;66;03m# Train PPO policy\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[0;32mIn[20], line 308\u001b[0m, in \u001b[0;36mCustomGymEnvironment.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    304\u001b[0m action_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m    305\u001b[0m     action, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# print(\"Hii\",state_tensor.shape, action_tensor.shape)\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# Use LNN to predict the next state\u001b[39;00m\n\u001b[0;32m--> 308\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_tensor\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Use Reward Network to calculate reward\u001b[39;00m\n\u001b[1;32m    312\u001b[0m reward \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_model(\n\u001b[1;32m    314\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(next_state, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    318\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[20], line 218\u001b[0m, in \u001b[0;36mLNNModel.forward\u001b[0;34m(self, o, a)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, o, a):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# ic(o)\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# o = get_obs(o)  # ! edit\u001b[39;00m\n\u001b[0;32m--> 218\u001b[0m     s_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrk2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_trig_transform_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m     o_1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrig_transform_q(\n\u001b[1;32m    220\u001b[0m         s_1[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn]), s_1[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn:]), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m o_1\n",
      "Cell \u001b[0;32mIn[20], line 210\u001b[0m, in \u001b[0;36mLNNModel.rk2\u001b[0;34m(self, s, a)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrk2\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, a):\n\u001b[1;32m    209\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m3.0\u001b[39m  \u001b[38;5;66;03m# Ralston's method\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     k1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mderivs\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     k2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mderivs(s \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m*\u001b[39m k1, a)\n\u001b[1;32m    212\u001b[0m     s_1 \u001b[38;5;241m=\u001b[39m s \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdt \u001b[38;5;241m*\u001b[39m ((\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2.0\u001b[39m\u001b[38;5;241m*\u001b[39malpha))\u001b[38;5;241m*\u001b[39mk1 \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m(\u001b[38;5;241m2.0\u001b[39m\u001b[38;5;241m*\u001b[39malpha))\u001b[38;5;241m*\u001b[39mk2)\n",
      "Cell \u001b[0;32mIn[20], line 205\u001b[0m, in \u001b[0;36mLNNModel.derivs\u001b[0;34m(self, s, a)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mderivs\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, a):\n\u001b[1;32m    204\u001b[0m     q, qdot \u001b[38;5;241m=\u001b[39m s[:, :\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn], s[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn:]\n\u001b[0;32m--> 205\u001b[0m     qddot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_acc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqdot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat((qdot, qddot), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 198\u001b[0m, in \u001b[0;36mLNNModel.get_acc\u001b[0;34m(self, q, qdot, a)\u001b[0m\n\u001b[1;32m    195\u001b[0m c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbjik,bk,bj->bi\u001b[39m\u001b[38;5;124m'\u001b[39m, dM_dq, qdot, qdot) \u001b[38;5;241m-\u001b[39m \\\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbikj,bk,bj->bi\u001b[39m\u001b[38;5;124m'\u001b[39m, dM_dq, qdot, qdot)\n\u001b[1;32m    197\u001b[0m Minv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcholesky_inverse(L)\n\u001b[0;32m--> 198\u001b[0m dV_dq \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreacher\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mjacrev\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_V\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m qddot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(\n\u001b[1;32m    200\u001b[0m     Minv, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_A(a)\u001b[38;5;241m-\u001b[39mc\u001b[38;5;241m-\u001b[39mdV_dq)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qddot\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:604\u001b[0m, in \u001b[0;36mjacrev.<locals>.wrapper_fn\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    603\u001b[0m     error_if_complex(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacrev\u001b[39m\u001b[38;5;124m\"\u001b[39m, args, is_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 604\u001b[0m     vjp_out \u001b[38;5;241m=\u001b[39m \u001b[43m_vjp_with_argnums\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_aux\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    606\u001b[0m         output, vjp_fn, aux \u001b[38;5;241m=\u001b[39m vjp_out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:399\u001b[0m, in \u001b[0;36m_vjp_with_argnums\u001b[0;34m(func, argnums, has_aux, *primals)\u001b[0m\n\u001b[1;32m    397\u001b[0m     diff_primals \u001b[38;5;241m=\u001b[39m _slice_argnums(primals, argnums, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    398\u001b[0m     tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_primals)\n\u001b[0;32m--> 399\u001b[0m primals_out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprimals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(primals_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(primals_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "Cell \u001b[0;32mIn[20], line 186\u001b[0m, in \u001b[0;36mLNNModel.get_V\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_V\u001b[39m(\u001b[38;5;28mself\u001b[39m, q):\n\u001b[1;32m    185\u001b[0m     trig_q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrig_transform_q(q)\n\u001b[0;32m--> 186\u001b[0m     y1_V \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1_V\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrig_q\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    187\u001b[0m     y2_V \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2_V(y1_V))\n\u001b[1;32m    188\u001b[0m     V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3_V(y2_V)\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.func import jacrev\n",
    "\n",
    "\n",
    "def get_obs(state):\n",
    "    return np.array([state[0],\n",
    "                    np.cos(state[1]), np.sin(state[1]),\n",
    "                    state[2],\n",
    "                    state[3]\n",
    "                     ])\n",
    "\n",
    "\n",
    "class LNNModel(torch.nn.Module):\n",
    "    def __init__(self, env_name, n, obs_size, action_size, dt, a_zeros):\n",
    "        super(LNNModel, self).__init__()\n",
    "        self.env_name = env_name\n",
    "        self.dt = dt\n",
    "        self.n = n\n",
    "\n",
    "        input_size = obs_size - self.n\n",
    "        out_L = int(self.n*(self.n+1)/2)\n",
    "        self.fc1_L = torch.nn.Linear(input_size, 64)\n",
    "        self.fc2_L = torch.nn.Linear(64, 64)\n",
    "        self.fc3_L = torch.nn.Linear(64, out_L)\n",
    "        if not self.env_name == \"reacher\":\n",
    "            self.fc1_V = torch.nn.Linear(input_size, 64)\n",
    "            self.fc2_V = torch.nn.Linear(64, 64)\n",
    "            self.fc3_V = torch.nn.Linear(64, 1)\n",
    "        print(a_zeros.shape, n, action_size, obs_size)\n",
    "        self.a_zeros = a_zeros\n",
    "\n",
    "    def trig_transform_q(self, q):\n",
    "        if self.env_name == \"pendulum\":\n",
    "            return torch.column_stack((torch.cos(q[:, 0]), torch.sin(q[:, 0])))\n",
    "\n",
    "        elif self.env_name == \"reacher\" or self.env_name == \"acrobot\":\n",
    "            return torch.column_stack((torch.cos(q[:, 0]), torch.sin(q[:, 0]),\n",
    "                                       torch.cos(q[:, 1]), torch.sin(q[:, 1])))\n",
    "\n",
    "        elif self.env_name == \"cartpole\":\n",
    "            return torch.column_stack((q[:, 0],\n",
    "                                       torch.cos(q[:, 1]), torch.sin(q[:, 1])))\n",
    "\n",
    "        elif self.env_name == \"cart2pole\":\n",
    "            return torch.column_stack((q[:, 0],\n",
    "                                       torch.cos(q[:, 1]), torch.sin(q[:, 1]),\n",
    "                                       torch.cos(q[:, 2]), torch.sin(q[:, 2])))\n",
    "\n",
    "        elif self.env_name == \"cart3pole\":\n",
    "            return torch.column_stack((q[:, 0],\n",
    "                                       torch.cos(q[:, 1]), torch.sin(q[:, 1]),\n",
    "                                       torch.cos(q[:, 2]), torch.sin(q[:, 2]),\n",
    "                                       torch.cos(q[:, 3]), torch.sin(q[:, 3])))\n",
    "\n",
    "        elif self.env_name == \"acro3bot\":\n",
    "            return torch.column_stack((torch.cos(q[:, 0]), torch.sin(q[:, 0]),\n",
    "                                       torch.cos(q[:, 1]), torch.sin(q[:, 1]),\n",
    "                                       torch.cos(q[:, 2]), torch.sin(q[:, 2])))\n",
    "\n",
    "    def inverse_trig_transform_model(self, x):\n",
    "        if self.env_name == \"pendulum\":\n",
    "            return torch.cat((torch.atan2(x[:, 1], x[:, 0]).unsqueeze(1), x[:, 2:]), 1)\n",
    "\n",
    "        elif self.env_name == \"reacher\" or self.env_name == \"acrobot\":\n",
    "            return torch.cat((torch.atan2(x[:, 1], x[:, 0]).unsqueeze(1), torch.atan2(x[:, 3], x[:, 2]).unsqueeze(1), x[:, 4:]), 1)\n",
    "\n",
    "        elif self.env_name == \"cartpole\":\n",
    "            return torch.cat((x[:, 0].unsqueeze(1), torch.atan2(x[:, 2], x[:, 1]).unsqueeze(1), x[:, 3:]), 1)\n",
    "\n",
    "        elif self.env_name == \"cart2pole\":\n",
    "            return torch.cat((x[:, 0].unsqueeze(1), torch.atan2(x[:, 2], x[:, 1]).unsqueeze(1), torch.atan2(x[:, 4], x[:, 3]).unsqueeze(1), x[:, 5:]), 1)\n",
    "\n",
    "        elif self.env_name == \"cart3pole\":\n",
    "            return torch.cat((x[:, 0].unsqueeze(1), torch.atan2(x[:, 2], x[:, 1]).unsqueeze(1), torch.atan2(x[:, 4], x[:, 3]).unsqueeze(1),\n",
    "                              torch.atan2(x[:, 6], x[:, 5]).unsqueeze(1), x[:, 7:]), 1)\n",
    "\n",
    "        elif self.env_name == \"acro3bot\":\n",
    "            return torch.cat((torch.atan2(x[:, 1], x[:, 0]).unsqueeze(1), torch.atan2(x[:, 3], x[:, 2]).unsqueeze(1), torch.atan2(x[:, 5], x[:, 4]).unsqueeze(1),\n",
    "                              x[:, 6:]), 1)\n",
    "\n",
    "    def compute_L(self, q):\n",
    "        y1_L = F.softplus(self.fc1_L(q))\n",
    "        y2_L = F.softplus(self.fc2_L(y1_L))\n",
    "        y_L = self.fc3_L(y2_L)\n",
    "        device = y_L.device\n",
    "        if self.n == 1:\n",
    "            L = y_L.unsqueeze(1)\n",
    "\n",
    "        elif self.n == 2:\n",
    "            L11 = y_L[:, 0].unsqueeze(1)\n",
    "            L1_zeros = torch.zeros(\n",
    "                L11.size(0), 1, dtype=torch.float32, device=device)\n",
    "\n",
    "            L21 = y_L[:, 1].unsqueeze(1)\n",
    "            L22 = y_L[:, 2].unsqueeze(1)\n",
    "\n",
    "            L1 = torch.cat((L11, L1_zeros), 1)\n",
    "            L2 = torch.cat((L21, L22), 1)\n",
    "            L = torch.cat((L1.unsqueeze(1), L2.unsqueeze(1)), 1)\n",
    "\n",
    "        elif self.n == 3:\n",
    "            L11 = y_L[:, 0].unsqueeze(1)\n",
    "            L1_zeros = torch.zeros(\n",
    "                L11.size(0), 2, dtype=torch.float32, device=device)\n",
    "\n",
    "            L21 = y_L[:, 1].unsqueeze(1)\n",
    "            L22 = y_L[:, 2].unsqueeze(1)\n",
    "            L2_zeros = torch.zeros(\n",
    "                L21.size(0), 1, dtype=torch.float32, device=device)\n",
    "\n",
    "            L31 = y_L[:, 3].unsqueeze(1)\n",
    "            L32 = y_L[:, 4].unsqueeze(1)\n",
    "            L33 = y_L[:, 5].unsqueeze(1)\n",
    "\n",
    "            L1 = torch.cat((L11, L1_zeros), 1)\n",
    "            L2 = torch.cat((L21, L22, L2_zeros), 1)\n",
    "            L3 = torch.cat((L31, L32, L33), 1)\n",
    "            L = torch.cat(\n",
    "                (L1.unsqueeze(1), L2.unsqueeze(1), L3.unsqueeze(1)), 1)\n",
    "\n",
    "        elif self.n == 4:\n",
    "            L11 = y_L[:, 0].unsqueeze(1)\n",
    "            L1_zeros = torch.zeros(\n",
    "                L11.size(0), 3, dtype=torch.float32, device=device)\n",
    "\n",
    "            L21 = y_L[:, 1].unsqueeze(1)\n",
    "            L22 = y_L[:, 2].unsqueeze(1)\n",
    "            L2_zeros = torch.zeros(\n",
    "                L21.size(0), 2, dtype=torch.float32, device=device)\n",
    "\n",
    "            L31 = y_L[:, 3].unsqueeze(1)\n",
    "            L32 = y_L[:, 4].unsqueeze(1)\n",
    "            L33 = y_L[:, 5].unsqueeze(1)\n",
    "            L3_zeros = torch.zeros(\n",
    "                L31.size(0), 1, dtype=torch.float32, device=device)\n",
    "\n",
    "            L41 = y_L[:, 6].unsqueeze(1)\n",
    "            L42 = y_L[:, 7].unsqueeze(1)\n",
    "            L43 = y_L[:, 8].unsqueeze(1)\n",
    "            L44 = y_L[:, 9].unsqueeze(1)\n",
    "\n",
    "            L1 = torch.cat((L11, L1_zeros), 1)\n",
    "            L2 = torch.cat((L21, L22, L2_zeros), 1)\n",
    "            L3 = torch.cat((L31, L32, L33, L3_zeros), 1)\n",
    "            L4 = torch.cat((L41, L42, L43, L44), 1)\n",
    "            L = torch.cat((L1.unsqueeze(1), L2.unsqueeze(\n",
    "                1), L3.unsqueeze(1), L4.unsqueeze(1)), 1)\n",
    "\n",
    "        return L\n",
    "\n",
    "    def get_A(self, a):\n",
    "        if self.env_name == \"pendulum\" or self.env_name == \"reacher\":\n",
    "            A = a\n",
    "\n",
    "        elif self.env_name == \"acrobot\":\n",
    "            A = torch.cat((self.a_zeros, a), 1)\n",
    "\n",
    "        elif self.env_name == \"cartpole\" or self.env_name == \"cart2pole\":\n",
    "            A = torch.cat((a, self.a_zeros), 1)\n",
    "\n",
    "        elif self.env_name == \"cart3pole\" or self.env_name == \"acro3bot\":\n",
    "            A = torch.cat((a[:, :1], self.a_zeros, a[:, 1:]), 1)\n",
    "\n",
    "        return A\n",
    "\n",
    "    def get_L(self, q):\n",
    "        trig_q = self.trig_transform_q(q)\n",
    "        L = self.compute_L(trig_q)\n",
    "        return L.sum(0), L\n",
    "\n",
    "    def get_V(self, q):\n",
    "        trig_q = self.trig_transform_q(q)\n",
    "        y1_V = F.softplus(self.fc1_V(trig_q))\n",
    "        y2_V = F.softplus(self.fc2_V(y1_V))\n",
    "        V = self.fc3_V(y2_V).squeeze()\n",
    "        return V.sum()\n",
    "\n",
    "    def get_acc(self, q, qdot, a):\n",
    "        dL_dq, L = jacrev(self.get_L, has_aux=True)(q)\n",
    "        term_1 = torch.einsum('blk,bijk->bijl', L, dL_dq.permute(2, 3, 0, 1))\n",
    "        dM_dq = term_1 + term_1.transpose(2, 3)\n",
    "        c = torch.einsum('bjik,bk,bj->bi', dM_dq, qdot, qdot) - \\\n",
    "            0.5 * torch.einsum('bikj,bk,bj->bi', dM_dq, qdot, qdot)\n",
    "        Minv = torch.cholesky_inverse(L)\n",
    "        dV_dq = 0 if self.env_name == \"reacher\" else jacrev(self.get_V)(q)\n",
    "        qddot = torch.matmul(\n",
    "            Minv, (self.get_A(a)-c-dV_dq).unsqueeze(2)).squeeze(2)\n",
    "        return qddot\n",
    "\n",
    "    def derivs(self, s, a):\n",
    "        q, qdot = s[:, :self.n], s[:, self.n:]\n",
    "        qddot = self.get_acc(q, qdot, a)\n",
    "        return torch.cat((qdot, qddot), dim=1)\n",
    "\n",
    "    def rk2(self, s, a):\n",
    "        alpha = 2.0/3.0  # Ralston's method\n",
    "        k1 = self.derivs(s, a)\n",
    "        k2 = self.derivs(s + alpha * self.dt * k1, a)\n",
    "        s_1 = s + self.dt * ((1.0 - 1.0/(2.0*alpha))*k1 + (1.0/(2.0*alpha))*k2)\n",
    "        return s_1\n",
    "\n",
    "    def forward(self, o, a):\n",
    "        # ic(o)\n",
    "        # o = get_obs(o)  # ! edit\n",
    "        s_1 = self.rk2(self.inverse_trig_transform_model(o), a)\n",
    "        o_1 = torch.cat((self.trig_transform_q(\n",
    "            s_1[:, :self.n]), s_1[:, self.n:]), 1)\n",
    "        return o_1\n",
    "\n",
    "\n",
    "class RewardNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim=256):\n",
    "        super(RewardNetwork, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)  # Predicts reward\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=100000):\n",
    "        self.buffer = deque(maxlen=max_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (\n",
    "            torch.tensor(states, dtype=torch.float32),\n",
    "            torch.tensor(actions, dtype=torch.float32),\n",
    "            torch.tensor(rewards, dtype=torch.float32),\n",
    "            torch.tensor(next_states, dtype=torch.float32),\n",
    "            torch.tensor(dones, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class CustomGymEnvironment(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Gym environment that uses LNN for state transitions, Reward Network for reward calculations,\n",
    "    and incorporates a replay buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lnn_model, reward_model, state_dim, action_dim, action_space, observation_space, device, replay_buffer):\n",
    "        super(CustomGymEnvironment, self).__init__()\n",
    "        self.lnn_model = lnn_model\n",
    "        self.reward_model = reward_model\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        # Define observation and action space\n",
    "        self.observation_space = observation_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        # Initialize state\n",
    "        self.state = np.zeros(state_dim)\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        # print(\"<<<\", self.state.shape)\n",
    "        if len(self.replay_buffer) > 0:\n",
    "            # Sample a random state from the replay buffer\n",
    "            sample = self.replay_buffer.sample(1)\n",
    "            # print(sample.shape)\n",
    "            if sample:\n",
    "                self.state = sample[0][0]\n",
    "        else:\n",
    "            # Otherwise, initialize randomly\n",
    "            self.state = self.observation_space.sample()\n",
    "            # print(self.state.shape)\n",
    "        # print(\"<<<2\", self.state.shape)\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        # Convert state and action to tensors\n",
    "        # print(action.shape)\n",
    "        state_tensor = torch.tensor(\n",
    "            self.state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        action_tensor = torch.tensor(\n",
    "            action, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        # print(\"Hii\",state_tensor.shape, action_tensor.shape)\n",
    "        # Use LNN to predict the next state\n",
    "        next_state = self.lnn_model(state_tensor, action_tensor).squeeze(\n",
    "            0).cpu().detach().numpy()\n",
    "\n",
    "        # Use Reward Network to calculate reward\n",
    "        reward = (\n",
    "            self.reward_model(\n",
    "                torch.tensor(next_state, dtype=torch.float32,\n",
    "                             device=self.device).unsqueeze(0)\n",
    "            )\n",
    "            .item()\n",
    "        )\n",
    "\n",
    "        # Check if the episode is done\n",
    "        done = np.abs(self.state[0]) <= 0.1 and np.abs(\n",
    "            self.state[1]) <= 0.01   # Example condition\n",
    "\n",
    "        # Store the transition in the replay buffer\n",
    "        self.replay_buffer.add(self.state, action, reward, next_state, done)\n",
    "\n",
    "        # Update state\n",
    "        self.state = next_state\n",
    "\n",
    "        return next_state, reward, done, False, {}\n",
    "\n",
    "\n",
    "def train_with_lnn_and_ppo(\n",
    "    model, env, custom_env, device, num_episodes, batch_size\n",
    "):\n",
    "    reward_optimizer = optim.Adam(reward_network.parameters(), lr=2e-6)\n",
    "    lnn_optimizer = optim.Adam(custom_env.lnn_model.parameters(), lr=2e-6)\n",
    "    reward_loss_fn = nn.MSELoss()\n",
    "    lnn_loss_fn = nn.MSELoss()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Collect transitions using PPO policy\n",
    "            action, _ = model.predict(state, deterministic=False)\n",
    "            state1, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # print(state.shape, action.shape)\n",
    "            custom_env.replay_buffer.add(state, action, reward, state1, done)\n",
    "\n",
    "        # Train LNN and Reward Network\n",
    "        if len(custom_env.replay_buffer) >= batch_size:\n",
    "            states, actions, rewards, next_states, dones = custom_env.replay_buffer.sample(\n",
    "                batch_size)\n",
    "\n",
    "            # Train LNN\n",
    "            lnn_preds = custom_env.lnn_model(\n",
    "                states.to(device), actions.to(device))\n",
    "            lnn_loss = lnn_loss_fn(lnn_preds, next_states.to(device))\n",
    "            lnn_optimizer.zero_grad()\n",
    "            lnn_loss.backward()\n",
    "            lnn_optimizer.step()\n",
    "\n",
    "            # Train Reward Network\n",
    "            reward_preds = custom_env.reward_model(next_states.to(device))\n",
    "            reward_loss = reward_loss_fn(\n",
    "                reward_preds, rewards.to(device).unsqueeze(-1))\n",
    "            reward_optimizer.zero_grad()\n",
    "            reward_loss.backward()\n",
    "            reward_optimizer.step()\n",
    "\n",
    "            print(\n",
    "                f\"Episode: {episode}, LNN loss: {lnn_loss}, Rewards Loss: {reward_loss}\")\n",
    "        # Train PPO policy\n",
    "        model.learn(total_timesteps=2048, reset_num_timesteps=False)\n",
    "\n",
    "\n",
    "tmp_path = \"./\"\n",
    "# set up logger\n",
    "new_logger = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "\n",
    "\n",
    "# Setup\n",
    "env = gym.make(\"Pendulum-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "print(\"Action Dim\", action_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# lnn_model = LNNModel(state_dim, action_dim).to(device)\n",
    "batch_size = 64\n",
    "n=1\n",
    "lnn_model = LNNModel(\n",
    "    env_name=\"pendulum\",\n",
    "    n=n,\n",
    "    obs_size=state_dim,\n",
    "    action_size=action_dim,\n",
    "    dt=0.02,  # Time step\n",
    "    a_zeros=torch.zeros(\n",
    "        batch_size, max(0, n - action_dim), dtype=torch.float32, device=device\n",
    "    ) if action_dim <= n else None\n",
    ").to(device)\n",
    "reward_network = RewardNetwork(state_dim).to(device)\n",
    "replay_buffer = ReplayBuffer(max_size=10000)\n",
    "\n",
    "action_space = env.action_space\n",
    "\n",
    "custom_env = CustomGymEnvironment(\n",
    "    lnn_model=lnn_model,\n",
    "    reward_model=reward_network,\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    action_space=action_space,\n",
    "    observation_space=env.observation_space,\n",
    "    device=device,\n",
    "    replay_buffer=replay_buffer,\n",
    ")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", custom_env, verbose=1)\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Train\n",
    "trained_model, trained_lnn, trained_reward = train_with_lnn_and_ppo(\n",
    "    model,  env, custom_env, device, num_episodes=10, batch_size=4096\n",
    ")\n",
    "\n",
    "# Save models\n",
    "# torch.save(custom_env.lnn_model.state_dict(), \"lnn_cartpole.pth\")\n",
    "# torch.save(custom_env.reward_model.state_dict(), \"reward_cartpole.pth\")\n",
    "# trained_model.save(\"ppo_cartpole_with_lnn\")\n",
    "model.save(\"ppo_cartpole_with_lnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ppo_cartpole_with_lnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: -1080.87 +/- 308.68\n",
      "Done  199\n",
      "Done  399\n",
      "Done  599\n",
      "Done  799\n",
      "Done  999\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "env = gym.make(\"Pendulum-v1\", render_mode = 'human')\n",
    "# 5. Load the trained model (optional)\n",
    "model = PPO.load(\"ppo_cartpole_with_lnn\", env=env)\n",
    "\n",
    "# 6. Evaluate the trained policy\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=3)\n",
    "print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# 7. Run the trained agent\n",
    "obs, _ = env.reset()\n",
    "for i in range(1000):  # Run for a fixed number of timesteps\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        obs, _ = env.reset()\n",
    "        print(\"Done \", i)\n",
    "    # if not i%20: print(i)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
